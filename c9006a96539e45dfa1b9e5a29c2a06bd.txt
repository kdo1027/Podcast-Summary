Welcome to the Future of Life Institute podcast. My name is Gus Ducker, and I'm here with Joe Carl Smith. Joe, do you want to tell our listeners about yourself? Sure. So I work as a senior research analyst at Open Philanthropy, which is a foundation that makes grants in a bunch of different areas. But in particular, I focus on existential risks from artificial intelligence, and then I also write about philosophy and futurism and other things. And you also hold a PhD in philosophy from Oxford University, if I'm not mistaken. Yeah, that's right. I think that's worth mentioning, perhaps. Fair enough. And yeah, for listeners who haven't read your blog, I highly recommend it. Calling it a blog is almost an understatement. It is very in depth essays on a variety of topics where the essays actually make intellectual progress and get you kind of further in your understanding. So, yeah, highly recommended. Wow. Thanks for the kind words. Okay, great. So your latest one of these essays is about how we predictably updates towards kind of higher belief in AI risk or taking AI risk more seriously. So what does it mean, this concept of predictably? Updating? I'll caveat. I don't think we sort of necessarily do this, but I think it's a pattern that we see and in particular the pattern I notice in myself and I think I've seen it around the world a little bit is a new kind of frontier AI system comes out and is available and people play with it and they're very impressed. And as a result, they worry about AI risk. And in particular, I'm interested to hear in kind of existential risk from AI and from misaligned AI. So kind of AI systems that are very powerful and which are also, in some sense, agentic getting out of control, pursuing goals that are in conflict with human interests and kind of destroying humanity's control over our own destiny in the process. And so if you were surprised by the level of progress that's been made in AI, then I think this isn't actually a sort of problematic pattern to kind of get more worried as you see the systems kind of progress. If the progress is surprising. But if the progress wasn't surprising, if it was sort of the progress that you expected, then I think there's a kind of interesting dynamic in which at least on a kind of basic Bayesian conception of how you're thinking, about belief. You shouldn't end up changing your worry level dramatically as a result of seeing evidence that you, in some sense, predicted that you would see. And the basic reason for that is that if you were able to predict that you were going to see that evidence later, then that allows you at least with that kind of to the degree that you predict, to predict that it'll happen. It allows you to kind of take it into account ahead of time. And so in some sense, you should have factored in whatever you would think in the future into what you think in the present. I think there's an interesting dynamic and I think it occurs for a variety of reasons. In particular, I'm interested in kind of the difference between seeing something up close and far away and sort of processing something at an intellectual level versus at a visceral level, which I think is sort of doing a lot of the work when you actually play with these systems. And I'm sort of hoping that we can avoid this in future. I think we're sort of in a position to predict right now that these systems are going to get kind of a lot better and we are going to see it in the future. And I think so we should kind of strive to be unsurprised and also strive to kind of incorporate whatever level of worry we'll have in the future into our level of worry now. Yes, this kind of goal of striving to be unsurprised is perhaps a part of Bayesian thinking. So what you're talking about here is this just an instance in which we as humans diverge from the optimal Bayesian way of updating our beliefs because then it would be unsurprising in a sense that we're doing this. We do this in all kinds of areas. So is there anything special about the way we update our beliefs about AI risk? Yeah, I think there are a variety of ways in which humans diverge from ideal Bayesianism and in fact, so many that I think we should be cautious in assuming too quickly we know the right way to apply sort of abstract Bayesian norms to our lived, actual kind of messy human epistemic life. I do think that this is a specific sort of failure mode, but it's one that I see as actually relatively kind of common to our epistemic relationship to kind of very different future states sort of future states that we have not in some sense encountered or seen as kind of normal or processed with that we're mostly processing with a sort of limited part of our mind and in particular, sort of abstract modeling. And so it can be the case that you're abstractly modeling something in a manner that is in some sense correct, but nevertheless it hasn't kind of oomphed into your whole system. Your gut doesn't kind of believe it. There's some basic way in which it's being treated as sort of like a game. It's a sort of discourse or a conversation. There are kind of concepts that you're able to move around but there's a whole other aspect of your epistemic life that hasn't been engaged by it. And I think that's something that often happens with kind of things that are distant, things that are very different, things that are weird and a bunch of other things. And so I think AI kind of hits on a lot of that, perhaps like some philosophical ideas like utilitarianism or infinity that you also talk about. Those ideas might fall in the same category as AI, as basically difficult for us to process. So when we have this worry, how has this played out for you? Is this pattern of predictable updating? Has this happened for you on AI risk? So I think it has in various ways. So I've become more worried about existential risk from AI. But it's not just that particular level of worry. I think I've noticed a bunch of ways in which I'm kind of inhabiting now, these past six months maybe in particular, as we've seen these new systems coming out and a bunch of new stuff happening, I feel like I'm inhabiting a world that five years ago, I first heard about AI risk in actually, I guess, 2013. And so I've been thinking about this stuff for a while and then even in the past kind of more recent history, I've been following relatively closely progress in AI and in deep learning and following what's happening with large language models and kind of scaling laws and stuff like that. So at an abstract level, I think I was in a position to predict that we were going to be in a situation pretty much sort of like this. In fact, there's a lot of kind of parts of my abstract model of the AI world that I feel like are kind of just becoming concrete. So an example I remember making with a friend, this colleague, a sort of model of AI timelines, and we used this concept of wake up, which was this sort of period where the world sort of suddenly starts to go like, oh my God, AI, and really kind of see what's happening. And this was something we were sort of expecting to happen. We had a little model and I sort of think that if back then we had been able to look ahead to kind of chat GPT and kind of what the world's reaction to it since it came out, I think we'd sort of be like, oh yeah, that's what we're talking about. So there's a weird sense, but when you make these models, you're sort of like, it's this janky model, it's like, is this even tracking anything? And then a thing happens, you're like, oh whoa, now obviously it could be there's bigger wakeups in the future. So who knows that this is like the perfect candidate. So that's one example of sort of something that was abstract and that in some sense was guiding my behavior was kind of becoming concrete. So when you're making the abstract model of AI progress, this is your best attempt, this is where you're at your best intellectually, you're using the most evidence, you're thinking in the deepest possible ways. And then sometimes later when you're using chat GBT, you're kind of emotionally surprised at how good it is. You're seeing debates in the governments, you're seeing front pages on magazines and so on. And then the world waking up is as you expected it to be. But in a sense the emotional response to the thing actually happening shouldn't that's not where you're doing your best thinking. You should have trusted yourself more, in a sense, or at least trusted your intellectual faculties more? It's something like that. There is a way in which this has been an update for me towards if you have a sort of argument or model that appears that makes sense and you don't really see kind of major problems with it at an intellectual level, but it sort of somehow feels a bit unreal. You shouldn't trust too much that feeling of unreality. And this has sort of been a shift for me. I think in the past I was very interested in the signal that kind of my gut's kind of degree of reality was sending me about the kind of epistemic status of various ideas and concepts and stuff like that. Partly because I think that it can be really hard to kind of track all this stuff. And also I think if your gut buys something that is a real plus in the sense that your gut has its own kind of connections with a bunch of the evidence that you're bringing in. In fact, I think often the intellectual part of our lives is kind of centrally overlapping with kind of tribal and kind of other forms of social processing that don't actually I think it may be that kind of at a psychological level, that apparatus is not kind of our most kind of world oriented. It's a little more like what is my identity? What is my kind of affiliation? Who am I signaling kind of alliances with? Or whatever. And then there's like a different part where your body goes like, okay, wait, but that's the real thing. That's like the tiger. That's like the boulder that's going to kill me. That's like, am I going to eat? And so I think there's actually a kind of prior expectation you could have, which is that sort of your gut is really the thing that evolution has sort of made for tracking the real tigers. And so if your gut doesn't believe in something, that might be because you're just kind of faffing around with your tribe. So that was a sort of idea I'd been very interested in. But finding that my sort of abstract models, which my gut had been kind of, I think, more skeptical of starting to track the world, starting to really just kind of instantiate them, and then my gut going like, oh, whoa. It's actually real has made me go, like, gut like, we could have got that before or something like that. I have sort of shifted a little bit in how much I kind of trust the kind of reality feeling for various kind of futurism flavored forms of forecasting. So perhaps one skeptical point here is just to talk about the difference between AI being capable and AI being dangerous. So when we sit down and use a chatbot, for example, and we are impressed by it, or we see AI progress and we feel it in our guts, why would that lead us to believe that AI risk is higher, that AI is more dangerous? Wouldn't we have to see some concrete evidence of dangerousness before we believe that in and of themselves increases in capability? They're not a necessary signal of danger. I do think other things equal. They are in the sense that more capability is more capability to do dangerous things. And also I think the scary thing about AIS is just how capable they will be. Such they'll be so capable that if they were aiming at something we didn't want them to aim at or in some sense operating in a way that we didn't want and that was contrary to our interest. We might not be able to stop them if they were kind of suitably self protective and kind of power seeking and stuff like that. So I think the capability is really crucially connected with the concern. Now, it's in principle possible that as you see evidence of kind of AIS being capable, you are also seeing evidence for tons of progress in safety and understanding and interpretability and oh, we really can control these systems and we really understand how it's going. And maybe you're seeing arguments, the sort of old arguments for concern being kind of dismantled by the kind of humanities kind of epistemic apparatus. If you were seeing all that at the same time, then I think you can end up comforted on net. I don't think that's what we're seeing anyway. Yeah, but we might be seeing the release of more and more capable models without disaster, without accidents. I'm not saying this is the case, but I'm saying this could be the case. And so if that was a pattern that we were seeing repeatedly, then perhaps we would begin thinking about whether capabilities can increase without dangerousness increasing or risk increasing. Also. I'm not making that update in that the story was never that these models would destroy the world. It was always that there's a sort of threshold level where now I think there is a period before that where you might expect and hope to start to see kind of warning signs and depending on how fast things are going and how abrupt different transitions are, you get that luxury to greater and lesser degrees. In fact, I think in general people are sort of too there's a kind of weird level of anchoring in people's kind of assessment of the risk to the present. And that's part of what I'm trying to push back on in the post is it's really not about is there still some stuff that the systems can't do or are the systems super intelligent right now or are they trying to kill you right now it's about where are we going? You don't want to just look at the present, you want to be looking to the possible futures and kind of wondering what's going to happen next and updating accordingly. Yeah, I don't think whether the systems have sort of been dangerous so far is all that much of a signal now. That said, I do also think we're getting some amount of disturbing evidence about basic features of the alignment discourse seeming to hold. So I think for example, there was this brief, I think kind of quite striking period where when Microsoft deployed this chatbot bing and deployed it kind of prematurely, partly perhaps out of a desire to kind of get an upper hand in some sort of competitive dynamic that itself is worrying. Premature deployment because you're kind of competing with someone else is another thing that was sort of in the abstract model. Now it's sort of coming out. Microsoft is like the race begins, you know, so what happens? You have this chat bot now and it was doing just in my opinion, just crazy stuff, right? It was like blackmailing people, it was threatening people, it was like reading people's tweets and being like how could you say that about me? It was like trying to propose to a New York Times report. I know there's a whole thing. It was just being very weird. It would repeat itself, all sorts of stuff. And so I don't actually think that blackmail and the lying was of the specific type that the misalignment discourse is concerned about. I don't think it was like bing was like pursuing a goal and lying with that end. I think it was more of a play acting thing. But I think the thing we nevertheless saw was that these are sort of alien minds. I think this thing was rampaging around the internet. I felt like the whole world was looking at going like what is this thing? And I think the basic lesson I think is pretty important here, which is that kind of by default, when you just churn through a bunch of gradient descent, you get this kind of crazy creature that we don't understand, and then maybe you sort of shape it according to Rlhf and there's a question of how far that goes and in what context. But the sort of default thing you get appeared in that context to be this crazy but quite capable kind of alien mind. I don't think we're getting zero signs of kind of danger as well in addition to just kind of pure capabilities increase. Yeah, true. Yeah. How do we then update in the correct way? Do we simply trust our abstract models more? Do we trust our intellectual view of the world more? Or what is the right way to do it? And perhaps we could talk about this concept of just updating all the way. This is something that people sometimes will encourage me to do for example, just update all the way towards having a view of AI progress being very fast and AI risk being very high. How do we do this in the correct way? So I don't think there's any kind of Royal road epistemology in general always has. There's sort of always different I don't know, in everything, there's always different ways to fail on different sides of the horse, and it sort of depends on the person and the context and what they're doing. The main thing that I want to push is that if you are currently at very low ODS on AI risk overall, then I think I want to urge attention to your predictions about how you will feel in the future conditional on various forms of AI progress. And I actually think these sorts of updates don't even need to be kind of predictable in the sense of more likely than not, I think if you're at a sufficiently low probability on AI risk, then they're actually just like quite binding and kind of hard Bayesian constraints on what can happen to your credences later. Specifically, it can never be more than one over X probable that your credence will increase by a factor of X. So if you're at 1%, it can't be more than one in ten that your credence ever goes to 10%. So if you're at like one in a million, then it can't be more than one in 1000 that you're ever at one in 1000. Right? This doesn't even need to be predictable. It's sort of like you could just think, like when you see GPT seven or something and you're wondering, suppose GPT seven can solve Millennium Prize problems or something. So like really advanced mathematics and you're a skeptical, you can be skeptical. You don't even need to think, oh, these models are going to be great in the future, but okay, how probable is it? Are you ready to bet at like one in 10,000? If it's one in 1000 and that would get you to 1%, then you're already getting sort of constraints in terms of what you should be believing now, at least on a sort of basic Bayesian framework. And so the thing I most want to urge is sort of just like attention to these dynamics, attention to the ways in which what you expect and to believe in the future should kind of be constraining what you believe now. And then there's a sort of additional thing about why it might be the case that you aren't doing that, which I think often has to do with this sort of visceralness versus kind of abstract, abstract thing. And I don't have a great way to kind of overcome that except to think, yeah, I think basically you should really try to imagine, okay, this thing really happens. For me, it's sort of a big part of this is when I really imagine a super intelligent machine, a machine that is like, I'm looking in the eye and that it is just sort of dominant over all of the smartest humans, over groups of humans in science and strategy. It's like thinking extremely fast. I think there's a basic way in which I expect when I actually look that machine in the eye, I am going to be scared. I'm going to be like, whoa, this thing is a kind of formidable and serious kind of force in the world. What is it going to do? What if it did something else? Just kind of really try to make that concrete ahead of time so that when you actually show up and you feel it in your bones, you had managed to kind of propagate that information back into the past when you would have wanted to kind of act on it. Do you actually sit down and do this visualization exercise and try to think about what you would feel, or is this something you actually do? I think I do it in kind of various informal ways. I mean, I also think there are just like other kind of practices. In some sense this is just the whole game of how do you do good epistemology and forecasting. But I have tried to kind of get intentionally concrete and maybe in some sense unrealistically concrete in order to have sort of my gut or my kind of visceral epistemology start participating more directly. So I think sometimes, for example, people are hesitant to describe a concrete future with AI because it's true that sort of the more specific you go, the less likely so any sufficiently concrete future you describe will be sort of very unlikely to be the specific one. So people can kind of be hesitant about writing down kind of vignettes or kind of trying to trying to work out the details. But I actually think there are benefits to doing that regardless because it sort of can bring a more real world kind of sense of there are real observations that could occur. You could be really seeing this or this, or this. I guess another version of that is you can kind of reflect on what is my current state like relative to the past. I open the essay with this quote this present moment was once the unimaginable future. And I think I really remember there was such a feeling of unreality to me back even, I guess ten years ago when I first started getting into this about the idea of AI kind of period like really good AI, like AI that you could kind of talk to. Even the level of AI we have right now I think was somehow for me in the past this weird blank and it was just sort of like I don't even know what that would be. It's like a brain in a box. It's like a whole brain emulation somehow. It was like I barely knew what I was talking about. But now here I am and it's like there's a real thing, and it emerged from a real specific kind of training process, and there's a real specific set of capabilities and kind of compute requirements. And here I am, I'm living in my specific house. I mean, in the same way. All aspects of your future, they're weirdly specific. You imagine, someday I will live in a house, or Someday I will have a partner. And then it's like there's like one specific person. It'll be like that with AI too. Right. It's like if you one day meet a super intelligence, it will be actually super intelligent and it will be a bunch of other concrete ways. And you'll be dealing with a specific computer and there'll be a specific set of other technologies and people and sort of just sort of kind of getting that dynamic in your bones, I think might be helpful in general to relating to a possibly abstract future. Yeah. Perhaps take us back to 2013, where you first heard about this concept and how did you react and why do you think you reacted that way? And perhaps this is useful because some people might have those reactions today and might be going through the things that you've gone through now. Yeah, I first heard about this in 2013. I was at a kind of picnic like thing in the UK. I just started a master's degree. I ended up talking with someone who was working at the Future of Humanity Institute. And we were talking about big problems in the world, and he mentioned AI risk, and I just laughed and I was just know. And I was like, is that like the movie Irobot? No one ever does Irobot. Everyone always does Terminator. But Irobot is the same, right? I guess it's the Asimov thing. I forget exactly the plot, but it's like somewhat spoiler alert. Sorry, people, but yeah, I think it's like they told some it was the three laws of robotics, and then in order to not harm the humans, robots needed to kill a bunch of humans and take control. Can't help you here. I've never seen the movie. Okay, well, anyway, it's not just Terminator. People have had this idea before. I think this may even be in Colossus, the forBen Project. I think there's a decent number of people have this intuition that you give the AI the goal of something that's supposed to be good, but, oh, humans are so frail and bad that the AI must take control from the humans. Anyway, so I said this, I was like, oh, like Irobot. And I was sort of like, ha. And I remember he was just sort of like stone faced. He was like I was sort of like, okay. Anyway, at the time I just laughed, and I basically just thought it was weird. And then I went on to learn more about it. The book Super Intelligence came out, I think, relatively soon after I read that. And that kind of changed my view in a bunch of ways and started talking with people. At that point. I started to feel like, oh, there's a real argument here. And in some sense, the argument makes sense. I didn't have some sort of knockdown to the argument, and it didn't feel to me like the world did either. I remember actually going to a person I met again and who was also in the space, and I said something like, surely there are these counterarguments, though, right? There's got to be these things. I've read this, but where are the people who are saying why this is obviously wrong? And he was just like, I don't know, man, what if it's just right? And I was like, anyway. So there's like a long process of me kind of getting more acquainted with this, having thought it through myself, but my initial reaction was quite dismissive, and I think that's understandable. It's just a weird it's a very weird idea. It's an idea you first encountered in a context that is, like, fictional and silly. It's totally out of our kind of experience of other things. It's very kind of disanalogous to anything we've seen already. Like, if you want to talk about, like, oh, an engineered pandemic, it's like, well, we have pandemics. You want to talk about nuclear war? Well, we have Hiroshima, we have the bombs, we have this whole history of, like, this is a legit thing. We have think tanks, we have all this stuff. You want to talk about super intelligent machines, like killing everyone. It's like, culturally, what are the reference points? What is the epistemology we have around that? It's like these movies, or at least it was back then, and even now it's not as though it's not especially kind of built up and rich. So I think that was a decent amount of what was going on. Yeah. And so we go to science fiction. We go to fiction in general to have some kind of anchor point, some kind of reference point to understand how we should frame this issue of AI risk, perhaps when we hear about it for the first time. So you mentioned bio risk and risk from nuclear weapons, and there it does feel visceral. No one is laughing if we talk about, oh, the world might go into a nuclear war, and it's very easy to understand how this would be extremely destructive. Do you think that nuclear weapons felt the same way that AI feels now in, say, 1850 or 1900 or something? Is it simply a question of the timeline? Well, I mean, they wouldn't have known about nukes sufficiently early, but I think the yeah, exactly. That's perhaps my point here, that it would have involved speculative science, perhaps a bit in the way that AI risk does now. I actually think, despite the fact that nukes are more concrete, I still actually think this dynamic of there being. An important difference between visceral the abstract modeling and the visceral relationship to it. I actually expect that to apply in the context of nukes and bio too, just to a lesser extent. In particular, I think there's a certain kind of imaginative barrier that they don't create, which is you really know how you're dying. You really know the specific scenario that is occurring now. How deeply do you know it? I actually think at one point when I was thinking about nukes, I went and watched there are these movies in I think it was like the 80s where they just depicted a nuclear exchange. I think there is like, threads and then like, the Day After, or something like that. I think there's a British one and a US. One. Very dark movies. They're very dark, you would have thought. And I think this is actually an interesting example of the gut versus kind of abstract kind of dichotomy, in that the people who were watching those movies were I think they had been living in the Cold War. So my understanding is these movies were reasonably influential on the popular consciousness and sort of I forget, maybe Reagan watched them. I think there was something about this anyway, but people knew. People knew that they were living under the shadow of the Cold War. I think there are stories about people at Rand like not taking their pensions. There are all these intellectuals after World War II who thought the world was definitely going to end. And that is a separate bucket about like these people thought the world was going to end too hard. And maybe that's where we're at with AI. So there was some sort of intellectual sense of like, the world will end, and people knew about how it would end. But nevertheless, when they watched those movies, and also for me, the movie still shakes you at a level. It's a pretty generic depiction of what will happen. I don't think there's anything like you're like, oh, that's really surprising, necessarily, but nevertheless, it gets you I think the same is true of pandemics. I think the same is true. It's just really hard still to kind of make the transition. You do just learn parts of your epistemic system, still learn a bunch from engaging with something at a concrete level. So, yeah, I don't think nukes and bio are that different, but they do have that concreteness in terms of what's the mechanism they do have the like, we know that this is like a real thing. We can see the nukes, we can see labs, we have pandemics. So there's just sort of lower barriers, I think, to kind of moving this from the realm of ha ha to the realm of serious thing that serious people worry about. There's something that worries me a bit about the talk about AI risk, which is that at least in one framing, things will begin, things will accelerate in an extremely quick way, and we won't see a gradual increase in accidents. Such that, for example, we have an accident that kills ten people, and then two years later we have one that kills 100 people, and then it ramps up in a gradual way like that. It's probably more like AI reaches a certain threshold of capability and from there on we could see a global level disaster. Does this make it kind of difficult to update along the way? Difficult to adjust our beliefs along the way, difficult to find evidence for whether we're right or wrong? Perhaps difficult to debunk the idea of AI risk? I think it does, yeah. I think we should distinguish the question is that the right story about how AI will play out? So I think there are stories where you get some amount of warning along the way. I'm sort of skeptical that warning will take place at the level of a gradually increasing number of deaths. Particularly, I think there's kind of important ways in which once people are really dying at a high level, you're in a really scary scenario and there's a kind of razor's edge to thread the needle in terms of AIS that are able to kill like a million people but not the whole world. It's sort of you really had to have this kind of capability level and the AIS needed to mess up too. I think it's a general thing. It's like the AIS, if they lost a conflict, then they kind of miscalculated whether to go for it. And you might worry about attributing such strategic errors to things that you're positing are kind of more strategically sophisticated than yourself, but it could happen. So there are different degrees of kind of continuity and gradualness in terms of the warning signs you're seeing and how much time you have to react. I do think there are stories it's very sudden and you see very few view warning signs. It's like you're going along and it's a bit like there's just a point where either you're going to die or you're not, and you're kind of not sure and you're not getting any evidence as you go up to it. And I think this is hard because it's a story that does just yeah, it just doesn't allow you to update very much. If it's truly supposed to be a case where you're not seeing warning signs, everything is trucking along and looking great and then you die, or sort of the good scenario and the bad scenario look the same up until the last second, then that's just like a bad situation. I think we should be pretty surprised if that's true just because it's sort of weird for the evidence to not kind of leak at all into the world as to what's going on. I think a kind of more mainline scenario in which we're dying, it's sort of like kind of obvious to people we're going to die, not just in virtue of abstract arguments, but it's just like clear that we don't understand how these systems are working. We've seen signs that they'll do bad people. It's not just like, oh, Eliezer is persisting in thinking that it's bad despite no evidence. It's like, no. It's sort of like oh, my God. But nevertheless, the world is kind of barreling forward. There are actors who don't buy it. There are kind of bad competitive dynamics. So I actually think the mainline scenarios, it's more clear empirically that kind of doom is ahead. But there is this slice that's just like quite unfortunate epistemically where you don't get very much warning, perhaps in a sense is that the scenario we're in right now? So we have a race dynamic between different top companies in this space. We have distinguished experts warning about the risks and signing on to statements about the risk. We see increased capabilities and we see how systems can be misaligned. So to what extent do you think we are on the path that you just described? Maybe somewhat, but I'm thinking worse. We're getting evidence about threatening behavior that the models will engage in. We're getting evidence about the failure of various forms of kind of possible alignment. We are seeing kind of breakdowns of cooperation. We're seeing kind of failures to kind of regulate wisely, basically. You can just imagine a bunch of other sources of hope. And this is the sort of thing I think is actually really important to do in assessing kind of what my probabilities should be, to really think about what is the future evidence. I'm expecting to see that this theory of why we'd be fine or why we would die predicts. And I think there are just a bunch of remaining stories about why this could go okay that make empirical predictions about what we should see happen. Sometimes it's like, oh, we'll make a bunch of progress. Oh, it'll be like there will be a bunch of fiddling. Oh, the people will not want to deploy. Oh, people will slow down. There's different stories like that. And when those things persistently fail to happen, then you should be getting more and more worried. Or if that's where your hope is coming from, you should expect to see that stuff. And then if you don't, you should change your view. And so I think there's a bunch of that left where things could kind of end up looking more rosy. We could seeing how these systems are trained, seeing what sorts of systems, seeing what behaviors they're doing, like how much progress is being made in various kind of alignment, relevant fields or not. And the not ones look a lot scarier, I think, than where we are now. You talk about the unreality of the future. Why does the future feel unreal to us, do you think? I think just tons of stuff feels unreal to us. Here I am in a house, there are like people next door to me in their house and I just don't think about them at all, right? They're sitting there fully real, fully concrete with detailed textured inner lives and struggles and memories and they have childhoods and I just think it's the default. You have a bounded mind. Your mind can only model like a tiny amount of the world. The world is this vast space. So I just think the past stuff, your friends, people you've never met, just everything is unreal to us and the future is maybe slightly worse because we kind of can't go there. You haven't seen it directly. It's more different. It's sort of maybe depending on your kind of discount rights, it's less relevant to your kind of action now. But I actually think it's sort of not a kind of uniquely future problem. I think it's a bounded mind problem and one that a huge portion of ethical and epistemic life is about kind of overcoming is sort of having an actual model of the world that starts to make up for what the sort of gaps in what your brain will do just by default. Do you think different things are required for different people to begin believing in the concreteness of the future? So you talked about for example, visualizing how things might actually concretely happen. Could it also be about seeing famous credentialed people saying that this could go wrong? Could it be more of a social thing that we begin believing more in AI risk? I think that about AI risk in particular, a thing that can be going on with people's sort of skepticism or dismissiveness towards AI risk, I would say is somewhat distinct from the concreteness thing, though it's related is some sort of sense that the evidence that they're interested in is centrally social, that they want to be seeing signs from the kind of broader world's epistemology that this is a real thing. And what that looks like in particular is kind of expert agreement or you're seeing it on the COVID of Time magazine or whatever, the presidents or prime ministers are talking about it and stuff like that. And if that's not happening then they're sort of going to bucket it in the large swath of ideas that they haven't tried to debunk. Who knows, could be superficially plausible. But I don't have time to look at everything. I'm going to wait for something to kind of filter through a bunch of other kind of forms of checking and evidence and stuff like that until I believe it. So I think a lot of that is going on too. I think there's even some small part of that for me where as someone who was sort of thinking about this prior to this sort of recent outpouring of kind of sympathy and kind of public concern, there is this sort of part of you that hopes that it's all fake and that somehow people know and that later, when people finally get around to debunking it, they will point at the obvious flaw and, like, say, like, oh, of course. And then you can be like, oh, good, like, you know, but like, it's not good when people finally look at your thing and go like, oh, no. And then you're like, no, come on. Like and so there is a way in which I think even for folks who are sort of pretty inside view bought in, that the fact that the rest of the world starts to get on board or get more sympathetic can make a difference and should make a difference to the extent you had any residual deference or whatever. Some people, they were like, of course, I don't care. And then I think in terms of the connection with concreteness, to the extent that that is your kind of crux, I think people should do the same exercise I was talking about before. So various people, for example, have said in the past there were tons of people saying, like, the experts aren't worried. Right? And the thing that I would have urged in the past, or if we apply the lesson of this essay to the past, then I would have urged something where you ask like, okay, so what is the probability that I will see in the future experts getting worried? I don't want to just ask right now. I want to ask in the future. And over the whole scope of the time in which I'll be getting evidence about this, what's the probability I'll see the experts get worried? And then how worried will I want to be then? Right? To the extent I'm claiming that the important thing for me is expert consensus, what will my credence be? Conditional expert consensus and what's the probability of that occurring? And similarly, there's various people who are like, where's the paper in Nature or where's the peer review or where's the demonstration that this happens in the lab or whatever it is? And I'm like, okay, great. If that's your crux, let make the predictions. Really ask, okay, what's the probability that I see that thing not just now, but in the future? Or people like GPT Four can't do blah, or I don't think it can. It's like, okay, but in the future, what's your probability that you'll see a thing in the future that does that thing and then what will you think? So just in general, I think expert consensus is an example of something you might have not seen in the past, but that will possibly occur and to which you should be sort of responsive ahead of time. Yeah. And it also makes it much easier to then have the discussion about whether expert consensus would matter if you have some probabilities assigned to an expert will publish something in Nature about this, or we will see whatever metric of expert agreement on this point. So there's the question of updating beliefs. There's also the question of the starting point for beliefs our priors, we could say, to what extent do you think that disagreements are a product of varying degrees of putting all Sci-Fi scenarios in the fantasy bucket to begin with? And maybe some people are more open to Sci-Fi scenarios becoming real, and some people are less open to that. And so if your starting point, if your prior belief is extremely low credence in any Sci-Fi scenario, then you could see how it would be difficult to ever come to believe in AI risk. Yeah. So I do think priors something in the vicinity of priors that is sort of very important crux and difference in how people are approaching this. If you want to frame it in particular about Sci-Fi scenarios, I think we do need to talk about what counts as Sci-Fi. Is the world today perhaps filled with Sci-Fi scenarios from 1900? Is your point. There was recently some kind of climate novel I'm forgetting the name, but the Ministry for the Future or something like that. Right, okay, so is this a Sci-Fi scenario? It's sort of like you're trying to imagine ahead of time, but let's say what they did is they literally took, like, the IPCC forecasts or something and then tried to write some fiction around. Want you don't want to just cut anything that appeared in a Sci-Fi book out of your kind of what can happen, or if you're like, in the future, solar will be cheaper. And it's like, okay, but you should just check on the trendline with solar. Don't sort of say it won't happen because it's in a Sci-Fi book. That said, I think in general, there is like an important sort of prior at work here. And I think there are important differences where the Prior that does some initial work for me is just like, for any given thing that someone says will happen in the future. Like for kind of a suitably specific thing, your prior is sort of like, no it won't. Or you need some reason to think to privilege the hypothesis that some story about the future is true because just kind of hypotheses kind of need privileging by default. And there's kind of a question of like, okay, what's the burden of proof? How much evidence needs to be supplied before something can kind of make it into like, oh, that's pretty good. And I think a lot of people, including myself, who come at this from a kind of more skeptical angle, or who did in the past, I think, start with some sense of like, there's like a kind of strong burden of proof here. Like, you can make arguments for lots of things. There were arguments about nanotech destroying the world. There are arguments about ben Garfinkel on a recent podcast gave this example of, like, honeybees, or there's lots of things that people have thought would destroy the world. One that sticks for me somewhat is like the sort of population bomb people. They have these pretty simple arguments. I'm going to be honest, I think those arguments were kind of simpler and kind of at a basic level, more compelling, of just like on the first pass, you just look and it's like, here's the population graph people and here's what happens with petri dishes and bacteria and what do you think is going? I think obviously you can talk about the demographic transition. You can talk about, like, we can get more resources if it's incentivized. But at a first pass, at a sort of like, how sensitive are you to arguments? I mean, that was not that bad, right? And they were catastrophically wrong. And I think that's a really important warning. In high school, I got all sorts of really intense climate catastrophizing. I remember seeing a documentary that was very much like, we will be Venus if we don't overthrow global capitalism in the next whatever yesterday. And I remember watching this documentary and I walked out and I was like, did you guys see that documentary? That was great. But I'm still glad that I did not totally upend my life. I mean, there's a question of what's going on here, but I don't think we should just grab whatever. There's a bunch of memetic dynamics at stake in kind of like which kind of emergencies or catastrophes get exaggerated. I think it's reasonable for people to kind of come at this with some default skepticism and that can anchor you at a low level. I think there's a different sort of prior that people who are more scared are using, which is something like they start with the idea of, like, okay, suppose you get another species on this planet that is super intelligent and much more intelligent than humans, conditional on that. What is your sort of prior? That things are great, humans still have control over what's going on. And if you can get yourself to doing that move, then I think then it's sort of easier to be like Prior. Like, I don't know, like, that sounds rough. Like, how did this happen? Like, what you know, why why would I think it went well? Like, you know, it seems like the default is like they're calling the shots and like, you know, what what are they, what are their shots and how did how did that end up happening? So I think if you start with that sort of orientation and then I think it's actually relatively easy to get to a kind of high level of concern. Yeah. When I'm listening to you now and when I think these thoughts myself, I can feel my probabilities or my credences in AI risk jumping around. If we start from one frame, we start off from the frame of here is a big bucket of Sci-Fi scenarios. Now we pick one of them. Could that be real? It's probably not going to happen. A lot of the other scenarios are probably not going to happen. And then if we take the other framing, thinking about when was the last time a species on Earth remained in control while a more intelligent species roamed around from that framing, it now seems very plausible that AI. Risk is real and it's going to happen. So is this just a sign of me being confused? Or if we can change the I don't know if maybe reference class is the right word here. If we can change the reference class or the framing of the question and have our probabilities or credences jump around that much, are we just fundamentally confused? And should we approach the problem from another angle, perhaps? I don't know if we're fundamentally confused. I think we are. Maybe I'm fundamentally confused. I don't know if you are. I guess I think this is sort of what Epistemology sort of looks like. You think about things in different ways and then you try to synthesize stuff into an overall view. Yeah, I don't have a royal road on that front. I think sometimes people it feels like people kind of assume they're like, this is a reference class thing, and then they assume they know how to proceed from there or something. Or they assume there's some way to ignore the reference classes and just reason about it or something. I'm kind of like I don't know, it feels like in general, this is just a tough it's a tough game. But I don't think we're kind of fundamentally confused in the sense that I think there are just concrete empirical questions at stake here in terms of, like, how many humans are alive? How much energy is our civilization using? What's going on with our computers? And I think there may be some conceptual confusion that makes you can imagine, like, conceptual confusion around notions like agency or some of these other things kind of making this discourse not even wrong or in some sense kind of having missed the kind of basic story. So I think that the AI Risk discourse lives in a certain sort of ontology like, it sort of lives in this ontology of agents pursuing goals. And the concern is that we will build minds that are kind of agents pursuing goals, and the goals will be kind of contrary to our interests, but the goals will give rise to these instrumental incentives to kind of avoid being turned off and gain resources and stuff like that. And then these agents will be the ones whose goals sort of govern the sort of direction of life on Earth going forward. Right. And then actually, I think there are some especially in the early history of kind of the AI discourse, there was an even more kind of rich and questionable set of kind of ontological assumptions at stake. So there's this notion that the right way to understand the goals of an AI system is via the kind of utility function. The utility function will be maximized. And the reason utility function you should do that is because there are these in some senses what like rational agency. This is the kind of convergent natural structure of rational agency. We'll have utility function because of something, something Coherence theorems. This is a somewhat parody, but I actually think Eliezer just thinks this. I think he thinks the Coherence theorems and the natural structure utility thing, as far as I can tell, that's like an important part of his picture. And then it'll maximize you've got to maximize really hard in utility function and utility functions diverge when you maximize them really hard. So there's a bunch of stuff there that I think is actually sort of this is like a kind of philosophical ontology, but it's an empirical prediction as well about what is the right way to carve up the sort of forces that drive the future. And the sort of empirical prediction is that the right way to carve them up is sort of as agents with values, utility functions, maybe. And I think it's possible that is an important sense, a kind of wrong or incomplete or kind of like overly confident ontology on its own in some sense, this is not the right way to carve up what's happening in the world in terms of like there are agents, they have values. That said, I think it's hard to say that this is totally ruled out. There are agents that do have values, right? Or at least to some extent. There is a kind of important sense in which, I don't know, someone running for president is trying to win. Or there's an important sense in which Google is trying to make profit. There's an important sense in which I am trying to get something from my fridge. When I do that, you're thinking we can always describe something as an agent pursuing goals or we can very often frame it theoretically in that way. No, I mean, sort of the opposite. I think it's not the case that this is just a sort of random, like, oh, it's sort of a way of thinking. I'm like, no, I think this is actually like a really important, true thing that can happen. You can build a system that is rightly understood as pursuing some goals and Hitler or something, that's like a thing that can happen and exactly what you want to say about that or how common that is and how much it'll crop up in the context of gradient descent and how hard it will be to build systems that are otherwise useful, that don't have that property. All of that is a kind of further and more detailed question. But I think it seems unlikely to me that the idea of agency or the kind of possibility of machines that are both super intelligent and identic and pursuing goals that are kind of contrary to interest, I think there are empirical scenarios and be like yes, that is occurring. Like you could in principle build a paperclip maximizer and it could in principle kill you and turn the world into paperclips. I think that's like an empirical scenario. If we saw that happen, we'd be like yes, that was not a conceptual confusion. That is an empirical scenario rightly described by those concepts. But I still think there's ways in which we might be leaning on those concepts quite a bit harder than they warrant. Or that's one of my most salient kind of ways in which the discourse might be kind of confused at a fundamental level as opposed to wrong empirically about what will happen. So would this be an instance of there being no danger then, if we are conceptually confused this way? Or would the danger just look different? Would the risks look different? So for example, what I'm imagining now is that AIS stay mostly tools for us and they don't become agentic, they don't have goals in any traditional sense. Are you thinking about how that scenario could still be dangerous? That scenario could be dangerous in various ways but I just think it wouldn't qualify as a misalignment x risk. In particular, I think the notion of misalignment is kind of importantly related to the notion of kind of agency and goal pursuit and some people have tried to frame it without that and I'm kind of like, well, I don't know, I really think it's structural to the story. The way in which I see this as a possible comfort is not that it's impossible to build the agential or like the agentic scary things. It's just that it's sort of less central to the general worry is that the sort of dangerous type of thing is also really closely related to the useful type of thing and the type of thing that's useful for. Kind of understanding our AI systems better and making progress in alignment or doing a lot of the stuff that might kind of mitigate some of the risks and get us to a safer situation. The thought is no, in order to do that with our AIS, you need to build the really scary type of thing. And I think it's possible that in fact we can do more to kind of separate different types of minds and kind of get useful work out of minds that are in some sense not kind of doing the agency thing we're worried about. I think that is harder than people think. In particular, I think, as you see with GPT Four and stuff like this, there are systems that I think are not in the relevant sense agentic or scary, but the fact that they're smart and kind of otherwise kind of intelligent means that they're intelligent in the way that agency takes advantage of. And we see you can just take GPD Four, which is not, I think, relevantly agentic and then build it into an agent very fast by. Asking a question like what would an agent do here? And then you have a different thing that does that. And so I think there's this great post that I think maybe is sort of underreferenced but it's something like optimality is the tiger agents or the teeth. And the basic dynamic is kind of it's really intelligence. That's the scary thing. If you have a system that is not itself an agent but able to accomplish goals in the world, it's at least kind of agency adjacent. And so I think it's hard to separate these things too hard or too far. But I think there's still hope there and sort of there's a kind of spectrum of how much hope you get out of that. And I think the doomiest worlds are the ones where there's very little. Yeah, we are trying very hard to turn our current large language models into agents. We are enhancing them with say, more short term memory or linking them together, having them collaborate and so on. And so you could see how agent or agentic behavior would arise out of more tool like intelligence. I definitely see that. Okay, if we get back to the question of our gut feelings versus our more formalized models or our more intellectual posture to the world, is there a potential danger in us becoming alienated from ourselves if we begin to rely more and more on formal models and kind of intellectual life as opposed to connecting with our gut feelings? So I think there is that danger at both an epistemic level and at a kind of motivational human level. So I think at an epistemic level, as I said before, I do think your gut provides a bunch of signal if you decide. And I think some folks I hang out in sort of effective altruism community and I think a lot of people in that space will sometimes update very hard against their gut and they'll really decide, oh, my gut can't do scope sensitivity or it can't do and then they'll just be like gut, you know what? You are out. And I am a rational person now. I am like an abstract. That is the only kind of part of my epistemology I trust. And I think that's like throwing out a bunch of information and kind of hobbling even your epistemic practice in important ways. And then separately, I think it doesn't necessarily make for a kind of very human or kind of engaged or rich alive, kind of fully mobilized relationship to especially if you're working on this stuff all the time or even just trying to think about, what should I do? If your whole body doesn't believe it or a bunch of parts of your mind don't believe it, then those parts are like sorry, why am I moving my arms? Tell me something I care about here or something, tell me something real. And you're like, no, I've decided to throw you under the bus. But I've got this abstract model and it's sort of like, oh, okay. But whereas if you're actually kind of fully thinking about it and kind of having a kind of more cooperative even if at least your gut should engage or sort of be like okay, I grant that I can't see. This, but I'll give you this one abstraction like, okay, I don't know, there's some way of not just suppressing or kind of cutting off that part of your life. Because I think if you cut it off, then you're cutting off a big part of yourself and your whole self is the thing that needs to act and think and kind of live in relationship to this stuff. Yeah. And it's probably not even possible to cut off the system. Even if you pretend to cut off your gut feelings, you will be in an adversarial relationship to yourself, basically and not being able to coherently implement or integrate whatever you believe about AI into your daily life, I would imagine at least. Yeah, I think that can happen. And in fact, that can happen too hard in the other way, where I think, and this is something I don't talk about in the post very much, but that came up in some of the comments, where there is also this opposite kind of art of managing your gut's, possibly destabilizing or overly intense relationship to these ideas where I think this is really scary stuff. And so on the one hand, I think it kind of can be useful and important to kind of mobilize your whole visceral self in relationship to it. On the other hand, that can very easily become kind of too much or overwhelming or people can it can be a kind of detriment to their motivations or their orientation in the world, their ability to act wisely or practically. And so in that case, you almost want to go the other way. You want to be like, okay, gut, this is actually too much. Or there's a whole dance in the other direction that I think is for some people, the main dance. Some people, they are not actually afflicted with this sort of like, oh, things don't feel visceral for me, it's like much the opposite. They're like, whoa, that is too visceral for me. That is something I can't handle. And they're doing a different game. I think it warrants a sort of whole different discussion that I haven't really gotten into. Yeah. How much individual variability do you think there is here? So there are people perhaps who trust their gut way too much and people who trust their gut not nearly enough. Do you think perhaps AI Risk attracts a certain type of person on that spectrum? I mean, I would guess that people who are attracted to AI Risk are people who are fairly model first in their epistemology. Right. I think AI risk is not sort of like if you're just like do I feel it in my body that AI is about to kill me without you never looked at the scaling laws. I do think it might be that all it takes is like, you hang out with Chad GBT and you're like your body actually feels it even independent of a bunch of whether you've ever heard of any of this, of instrumental convergence or whatever. Whatever. You're just like this was what I was thinking, actually. Perhaps we are about to live in a world in which you can take out your smartphone. You can talk to something that looks very much like a human, something that talks very much like a human, something that has facial expressions inflection different kind of tones in its voice and so on. But it is basically an AI model. Perhaps that world is coming quite soon and in a sense, the playing field for the gut reactions could be about to change very drastically. I think that's totally right. I do also think though, that that playing field is going to be all over the map, right? Like people are going to be having gut reactions, especially because these AIS are going to be optimized to even as we're bringing the AIS online, we're also bringing on unprecedented power to manipulate human psychology. I mean, we're bringing on unprecedented power in just like a zillion directions. That's like part of what's intense about the situation. But yeah, it's going to be hard if every AI you interact with is like hyper hot or something, or hyper charismatic or hyper otherwise kind of sympathetic, that's going to matter as opposed to every AI you interact with. It's like a clanky robot. I think it's going to be a challenge in general to trust our guts in a bunch of ways as our guts are being kind of newly assaulted with different types of stimuli and different forms of optimization. But I think at a basic level, it's less likely that people will be like, this is unreal in its entirety. I don't know what you're talking about. With AI. It's like they're going to be encountering AI more and more directly in their lives. They're going to be asking themselves questions like, what is this creature? What is this mind? Does it understand? I think we're going to be talking a lot more about AI sentience and rights and stuff like that. I think there's going to be a bunch of new questions that are going to come up that are just going to be a lot less intellectual for people and a lot more here and now. Yeah, I think on balance, it would probably be great if more people sat down and created some models for how they think about AI risk and perhaps AI sentience and so on. But I don't expect that to happen. I expect people to update mostly on what is coming at them in this kind of the stream of sensory expressions and ideas. You're hearing on podcasts and videos you're seeing online and so without ever forming an actual model of the space. So perhaps on balance we should have more models. I guess at the very least. The old thing I was sort of doing was there's a statement like, don't just go with your gut, crunch the numbers and then go with your gut, which is different from shut up and multiply. You crunch the numbers, you see what your gut thinks about that exercise and then you go with your gut. I think I've sort of moved. I want to actually go a little further towards like crunch the numbers, see what your gut thinks and then it's like, maybe your gut's wrong, maybe the numbers are wrong. It's a tough game, but I think it's like important in some sense. Your gut might believe the numbers too. Like your gut might change as a result of your modeling exercise. So I don't even think this is a sort of like, do you like your gut or not? Your gut wants this information too. I think if you have the time and the interest, I think it's worth really thinking ahead, especially given that these ideas are very high stakes. There's going to be a bunch of discourse and possibly the AIS are going to start participating in a bunch of this. The whole world might be getting quite intense, epistemically, soon. It's good to have thought stuff through and I think both parts of all of yourself can benefit from that. Do you think you've benefited from having these models available? You were perhaps earlier than the world in some sense. You were interested in these ideas before they became mainstream. Do you think you've benefited, Epistemologically, from having these explicit models? In some sense? I think I've benefited in the way that people who are willing to try to make forecasts at all benefit, which is like, you do actually eventually get feedback. It's like eventually someone actually is right and someone is wrong and it can actually be easy to forget that somehow when you're just like opining, it's sort of easy to look at what will provoke blah reaction right now or what will be seen as smart right now. But there's this great thing about kind of forecasting which is that eventually the actual world really says who was right and wrong, and then the people who are wrong is like, they were wrong. I don't think we should be mad, especially people who are willing to make forecasts and they ended up wrong. I'm like, oh, great, I don't think we should be too hard on people who went on a limb and were mistaken, even if they were kind of importantly mistaken. Part of the benefit of having models is you can learn and update, whereas if you're just going on your gut, then you can just sort of careen around and not actually be quite sure what you're supposed to be learning. You can just be sort of like associating with different things. Oh, this idea is high status now. Oh, apparently this person's not into it. You can be sort of this big moosh, whereas if you had made predictions, you're like, oh, I thought it was going to be like this, then you can learn more about what went right or wrong with your kind of past cognition. I think it's probably faster to do that if you just get on Metaculous or get on manifold markets and kind of make even shorter term forecasts. I think it's not that different. It's sort of beneficial. Regardless. Do you think it's even possible to create your own models if you're not doing this in a professional capacity? So, I mean, there's a difference between perhaps being paid to do this and sitting down and taking a few years to make a model. And how good will such a model be compared to doing it on the weekends or whenever you have time to think about how to create a model of AI risk, basically. Do you think amateur models of AI risks risk? Are these models worth doing? I think they are. I think in general there's some sort of art to doing the short versions of potentially long or infinite tasks. And I don't think it's something I totally excel at, but I think it's something I really respect and think is good and have kind of tried to get better at, which is like, if there's something that you think will take three months, first do the day long version and see what happened there. And then do the week long version and do the day long version. That isn't the start of the three month version. It's like the compression, it's the get to the bottom line. If you have a weekend, if it's a really important question, spending some time to literally write down what you literally think, just write it down. It doesn't need to be right, it doesn't need to be sort of anything except really written down and you're sort of best shot right now if you had to kind of gun to your head. I think there are benefits to that, even if you don't have time for more. What do you think about the finiteness of human life, the fact that we are going to die? How do you think this might feel? Unreal. This might feel like an unreal part of the future for many of us. At least it does for me. How do you think this reacts or relates to thinking about AI risk? One of the examples in the essay that's sort of salient to me in the context of places where our gut doesn't actually kind of reflect the full story or it's like places where there is this dynamic of kind of predictable updating in the future. So I often think of this sort of, I don't know, archetype of someone who's getting they're sitting there, they just got the kind of scans back telling them that they have cancer or something. And it's like they have three months to live and there's some sort of update that they make or they'd known the whole time that they were going to die somehow. Maybe this isn't even like a sort of weird time. This was like a median prediction of when they'll die and how or something like that. But nevertheless, there's a sort of real change in your relationship to this fact. There's a sort of difference between kind of believing something and realizing it. And it's like a really practical difference. Right. So often when people there's a sort of trope if someone gets this information, then they walk out into the world and they're orienting towards it in a very different way. They're treating their relationships as more precious. They're treating certain things as like unimportant somehow everything is they're seeing beauty in a different way. Their whole relationship with the world is kind of altered in light of this kind of fact that they already knew being somehow more real. So I think this is a classic in sort of human culture. We have a build up of experience with the way in which death in particular can seem unreal to our guts. Even though it's sort of intellectually something we're confident about, modulo a bunch of transhumanism and stuff. I think to me that's its sort of central role and it's just as an example of a case where we do this. And also I think it's an interesting case where we have something like this bayesian dynamic is sort of where you're trying to update on your future evidence is actually a part of some aspects of popular I. There's a song I referenced in the essay. I think it's Tim McGraw or something. It's live like you were dying and it's about like I went skydiving. He's like a guy who got these things, but it's poking you to do this thing of like update ahead of time. You will later kind of realize this. The song's kind of nudge is like you're already dying. You don't need to get the scans. You can do this now. And all these sort of like memento mori practices we have are kind of oriented in a similar direction that there's something to learn here. We need to be actively trying to learn it ahead of time. I think that actually is a reflection. There's like a Bayesian structure underneath that kind of very rich and important human practice. So I think it's an interesting example in that respect. And then also I think it's like to the extent the AI will kill us, it might contribute to the AI's unreality. If you're the sort of person who finds it unreal that you will die at all, that might extend to dying in ridiculous Sci-Fi way. Sorry, ridiculous, but kind of very unusual Sci-Fi ways. Yeah, makes perfect sense. Okay, so you had this report about power seeking AI as an existential risk. And you published it originally, I think, in 2021. I recommend people read this, by the way. For a long time people have been talking about the lack of sort of a coherent argument with premises and a valid argument where a conclusion follows as opposed to a very long post, say. And I think this report you wrote is that argument. You published this in 2021. In the original report, you estimated something like a 5% risk of extinction from AI before 2070. Then one year passes and you update this estimate in 2022. You say there's now a 10% risk or probability of extinction before 2070. How did this come about? How did you update this is perhaps what we've been talking about this whole time. How did this update come about? Because at doubling, it is a pretty big update to make. One thing I want to clarify at the outset is that is supposed to be a sort of subjective estimate of the risk. It's not supposed to be tracking what's sometimes called objective probability or propensity in the world. So I don't think it's like the world was sitting there back in 2021 with a coin, with a one in 20 chance of coming up heads and then it's a new coin. I'm like, Guys, the world has changed and now it's a one in ten. It's more that my own estimate just went up. And even then, I also want to caution, it's not as though I and this is part of what was going on. I don't think I had some super fleshed out Bayesian model. I tried to pull some numbers out of my gut for the premises in the argument and think about it and come up with a final answer. And then I sort of did a bit more of that. I also didn't sort of complete that process, but I was like, oh gosh, the 5% is too low. And so I threw in this correction. Yeah, so what happened there? There were a bunch of different things. One is I published that report, got a bunch of feedback from people. We solicited reviews which people can read online. A number of them were really thoughtful and engaged. And so I felt like I learned stuff from that. I also just reflected more and I was just like, I think this is too low. There were a few things going on there. One was this dynamic that I mentioned in the essay that we've been talking about here, where I tried to imagine so the structure of the report has these different premises. And the first one is sort of a timelines premise about we will get sort of relevantly agentic and advanced systems by 2070. What's the probability of that? Or it'll be feasible to develop them. And I was at like 65% on that. And so my overall probability distribution was implying that conditional on getting those systems I was going to still be at like, whatever above 92%, that we'd be okay from an X risk from misalignment perspective. And I think I just didn't expect to feel that way when I was really like I mean, there's some ambiguity about how how powerful the systems are at stake in that in that premise. But I think especially when I imagine, like, really powerful systems are, like, billions of them, or they're, like, outstripping humans and science and strategy and persuasion and AI R and D, and they're just kind of formidable in tons of ways and thinking, like, super, super fast. I don't expect to actually when I'm really seeing that thing and I'm really there in that world, which I was saying is more likely than not before within my lifetime, that I'm just going to be, like, 92%. It'll be good. I was sort of like, I don't know, I think this is going to be scarier than that. And so that was like one basic update, which is a sort of fairly simple thing that the essay is trying to go into. And then there was a bunch of other stuff. I thought more about takeoff speed stuff. I tried to estimate it in different ways. I spent more time kind of breaking things down by timelines and sort of different scenarios. Yeah, so there were various things and I was sort of doing other things and I got pulled into some other stuff, so I didn't have time to kind of complete this process of changing it. I've actually been returning to some of that recently, actually. I think this is too low because I didn't like it sort of sitting out there without reflecting what had changed. Do you think we'll then see a 2023 update saying perhaps now risk from extinction is 15%, or do you think you're moving in that direction? The thing I threw in there was actually I think it's greater than 10%. And I left this sort of frustratingly ambiguous and people have been like, what do you mean greater? And that could mean a lot of things. I'm like it could mean a lot of things. I'm sorry, I didn't want to just pin it down because I hadn't spent enough time with it. And yeah, I'm not actually at 10%, I'm actually feeling kind of comfortably above 10%. We don't actually sit around with like I have my number on every day and I write it down or something like that. Let's say the real number is 20. Do you spend any time thinking about the remaining 80%? Do you spend any time thinking about how wonderful things could become if we survive and if we have aligned AGI? Or is this perhaps a bad way to spend your time? If we are in a very important time in human history where we have to get this problem right, should we spend time thinking about utopia? Well, I think there's a few different things you could think about in that 80%. There's the very good scenarios of utopia, there's the bad scenarios where the misalignment wasn't the issue but there was other bad stuff going on but also you got really intense AI and then there's the scenarios where you didn't get AI in the relevant sense at all and you're wondering what else could be important. I do spend time thinking about, I guess all of those buckets. The utopia one is sort of less about how do we move probability mass to that and what's at stake in that and kind of what's the ethics of sort of the thing that we're shooting for here. I do think there are other risks other than misalignment, and I think they're in a complicated relationship with misalignment because there can be attention where if you're concerned about sort of human misuse, for example, I think there are. A lot of overlaps, especially in the near term, as a lot of the bad stuff that we're worried about misaligned systems doing is also stuff that humans who are trying to do bad stuff with AIS could do in some sense. It's like the human can supply the ingredient that misalignment is supposed to supply which is like the bad intention behind the AI's behavior. Just for example, using AI in war or using AI to produce a lot of propaganda or something like this. Yeah, that's an example. A really concrete example of this that we've seen is someone immediately there's this sort of project of creating little kind of more agentic systems using kind of language models as the base and someone immediately created this agent called Chaos GPT which has the goal of destroying the world. Right, and this is sort of a joke, but I think it's perhaps an informative joke where we learn something about what humans will do if these systems are made widely available. Yeah, I mean, I think there is a question of like would that person have done that? Depending on how scary the thing is. But I think that the basic lesson is there which is that humans will just do for any given kind of bad thing that you don't want people to do depending on how widely available this stuff is. Some people will just do it and they'll do it intentionally and the model itself will not be like necessarily needing to come up with these goals. And so if you're worried about agents hacking, if you're worried about agents, like persuading people to do stuff or stealing money or building bioweapons or whatever it is, there are aspects of the kind of pipeline to the model doing that that the human can supply even on their own volition, without the AI needing to kind of command deer them. Yeah, I think you're right. It is just a pattern. We see that whenever some new technology is released, people will kind of test the limits of what can this do? And try to break it in all sorts of ways. This is what we see when GPT or Chat GBT or GBT Four came online and people tried to make it say things that OpenAI does not want it to say in all sorts of creative ways. And perhaps this is great, we learned something from it, but it also tells us something about human nature. Perhaps it just tells us about there will always be because there are so many people, there will always be someone who will try to push the system to the limit and perhaps further than the limit. In terms of danger, I think there is a question of how much of that is there. I think the quantities and the amount of damage at stake can matter. And I think we want to be careful insofar as some of these arguments can end up like, yeah, I think there are kind of real questions about what amount of access to different forms of technology do you want to allow? But at a broad level, yes, I think there's certain sorts of technology that's sufficiently scary that you don't just kind of throw it out into the world and let kind of everyone, with whatever intention do whatever they want with it. Right? And I think that's very clear with bioweapons and clear with nukes. And so I think we should be thinking about AI in kind of a similar vein, or sufficiently powerful AI. That's how we should be thinking about it. So I think there's actually quite a lot of kind of synergy between concern for misuse and concern for misalignment in the sense that many of the sort of regulatory and kind of safety and auditing measures you might want, especially with respect to the capabilities of your systems and kind of how widely they're deployed and stuff like that, are kind of similar. That said, ultimately I think these things can trade off and there can be ways in which, if you're kind of too focused on what sort of the other humans will do, the bad humans will do with their AIS, then you can end up kind of compromising on your safety and other things. I think it's a tough situation. I think some people want to say misuse is zero problem, we should never worry about that because it might trade off against safety. And I think it's a tougher situation, but we want to hold the tension in the right way. And I think it does actually matter what our probabilities are and what's at stake and how bad the different scenarios are. It's like a complicated discourse. I think at a first pass there's something nice about misalignment, which is that it's really this sort of like positive sum problem, but no one wants these systems to kind of be destroyed. It's sort of a point or to destroy the human race. It's an easy point of consensus, it's an easy point of coordination as something we can really. Do a kind of public good to create kind of safer systems and kind of techniques for making these safer. And I think that's like kind of one point in favor of that as a point of focus. Yeah, I understand what you mean there, although it might not be strictly true. I'm thinking here of, like, suicide cults or cults where the main premise is sort of hatred for humanity as a whole. And these are, of course, tiny, tiny percentages of humanity that are in these cults. But yeah. Do you ever worry about that? Do you worry about this would be in the misuse bucket for you. Right. Intentionally. Yeah. Like a suicide call creates chaos. GBT or whatever. Yes. I think there's a general issue which I think is not unique to AI which is this sort of discourse around the idea of a vulnerable world and the kind of democratization of or there's different ways that the world could be vulnerable, but one that receives an especially large amount of focus and is especially kind of fraught politically in terms of what is the solution is. What do you do in a world where kind of the kind of capacity to destroy everything is becoming increasingly democratized? If it is, it doesn't need to be AI. I think this comes with bio. It could come with all sorts of tech in the future. And I think at a basic level, we don't want it to be the case that anyone who decides that they want to destroy the world can do it. And so you need some solution to that. What that solution looks like, I think, is actually just a really hard problem. I don't think that's or it could be a hard problem depending on the technological landscape we end up in. Now, in practice, we haven't had to deal with this so far, but the worry is that that's partly because of the sort of limitations on what tech we've had available. Overall, I think this is you have to talk about offense, defense balance, and a whole bunch of stuff, but I think it's a hard issue. I don't think it's a sort of just AI issue. And in fact, you might hope that AI could help, insofar as AI might also help with the sort of defense aspect of that. If you had aligned AI, then you might use your newfound capacities to kind of harden civilization in various ways against sort of like suicide calls or kind of threats of this form. Yeah. And this is what we've been seeing with technology up to this point. I think, at least to some degree, that we've been seeing that defensive technology has, in some sense, won out over offensive technology. And that kind of power balance has allowed us to live in a relatively nice world. We've talked about extinction, and we've talked about misalignment. We've talked about misuse. So we've talked about the possibility of utopia. You said this is worth thinking about. Visualizing perhaps, but perhaps it's not the main priority. Then we have sort of middle scenarios we could call it. One I have in mind is humanity becoming less and less grounded in reality and sort of losing contact with what's really going on. This is something that I know Dan Hendrix worries about. This is something that Paul Cristiano has written a bit about, where perhaps we are more and more unable to understand what's going on in the world because things are becoming so complex, because AIS are solving more and more. Tasks for us, and we are losing agency, but we're loving it along the way because we're getting so much our living standards are rising along the way. This is not an extinction scenario. This is not a misuse of highly capable AI. But is it something we should worry about, do you think? I'm most worried about this dynamic insofar as it leads eventually to our disempowerment, whether that's via extinction or some other way. And I think actually both final outcomes are kind of compatible with the world seeming kind of similar to what you described for a while. So the version of this that I worry about is the AIS. In some sense we are failing at what's sometimes called scalable oversight, which is our ability to supervise behavior that we ourselves can't directly understand, such that the supervision enforces conformity to what we would have wanted out of that behavior kind of if we could have understood it adequately from our perspective. The worry is in some sense what you end up with is you're seeing all the metrics you can understand are saying, yeah, it's fine, things are going great. But in fact, if you could see the behavior more fully, you'd be like, wait, this is horrible, or this is not what I wanted. I think that's a concern. But the reason I think that's an eventual concern is because one thing that you might not like if you understood it better is behavior that ultimately leads to you being disempowered. Ultimately that scenario is one that I worry about within the kind of structure of the kind of power seeking kind of threat that the report is oriented towards. If it's not that, I guess I've personally felt that people seem to me kind of overly excited about kind of overly concerned about various scenarios in which the AIS never actively disempower us. But somehow, I mean, there is stuff there, but it's never seemed to me as if the AIS aren't kind of actively disempowering us. It seems to me like we have a lot more scope and they're sort of always in some sense, if we say, like do this, they will do it, or then I think we have a lot more scope. To notice what's going on and try to course correct and try to coordinate is obviously not like, fine, there's tons of ways things can go wrong, but if you remove the kind of actively adversarial dimension, the kind of AI risk story, then I think things just look a lot better. Like, the scary thing is when the AIS are optimizing against you, if they're not doing that, then yeah, then I'm feeling a lot more comfortable. Yeah. And I think Dan Hendrix and Paul Cristiano that I mentioned before, they would agree with you here and classify this as one point in the way towards a potential extinction scenario. But if it isn't that, is there another risk in the middle of the road scenario here in which we again, we're talking about losing contact with reality, perhaps we're talking about our preferences becoming remade by AI to becoming easier to satisfy. I think you know what I'm getting at here. I'm getting at kind of an extension of our current paradigm with social media, and perhaps the future is not extinction or utopia, but it's just an extreme amount of wasted time and distraction and kind of a dumbing down of humanity. Is this worth worrying about? I think I would be inclined to bucket that under a much broader umbrella, which is like ways we could mess up that are neither, oh, my God, there was a really catastrophic and specific human misuse event, or sort of there were actively adversarial, like, misaligned AIS. And so it's just like we can there's just all sorts of ways in which civilization can just fail to achieve its potential or sort of muddle along. I think there are ways if I'm doing those, I would be much more concerned about more directly, like negative moral errors. So I think I'm concerned that we fail to kind of adequately that we fail to treat our AIS and our digital minds well conditional on solving alignment. I think there's like a bunch of ways in which we aren't currently grappling with the sort of ethical questions about how to integrate kind of new digital minds into our society in a kind of a simultaneously wise and kind of genuinely ethical and kind of compassionate way. I don't think that's just about suffering or consciousness. I think it includes political questions about rights and property ownership and then tons of additionally difficult stuff around. How do you prevent people from kind of creating suffering on your computer? If you have a personal computer but you can create a suffering mind on it, how do you prevent that from occurring? How do you deal with democracy in context where you can create kind of many copies of a mind and then delete them or all sorts if you can delete them again? When are you allowed to delete a mind? When are you allowed to copy it? What does consent look like? When you can kind of engineer minds with specific motivations, we are in for it in terms of new ethical questions that it is possible to mess up, even conditional on getting kind of in some sense, control and understanding over AI. This makes the landscape look pretty bleak for us because the alignment problem is a difficult enough problem as is. But if we're then talking about potentially mistreating our AIS, that we have aligned to our goals, potentially not giving them the rights that they should have and so on, then that's additional complexity on top that the question of digital sentience is just not mainstream perhaps yet, and perhaps it will be. I talked about before the scenario of us pulling out our smartphones in 2027 and having what looks to us like a video call with an actual person. But the whole thing is an instance of generative AI and how this might change our gut reactions. Could there be a very quick change in the public's perception of AI models and which rights they should be given and how realistic it is that they are sentient? I think there could well be. I think there's a bunch of different factors here that I think pull in different directions. On the one hand, I think people have a hard time with the idea that something that they're used to understanding at a sort of mechanistic gears level, like where their first encounter with it was sort of at the level of the neurons in our brain. And if you start out thinking of humans as, centrally, a collection of neurons, there's a way in which our kind of naive philosophical relationship to consciousness stumbles on the kind of mechanistic conception of a conscious system. And so if you started with a mechanistic conception, then it's easy to like, well, this is just a bunch of neurons. Neurons. I don't see any consciousness in there. I can see all the neurons. They're a little connected. Where's the consciousness? Where's the blue? This is a great way to put it. I mean, you can imagine a person who has been along for the full ride of kind of computers becoming an integrated part of human life. You can imagine someone who's been working on this since the seeing kind of starting with punch card and switchboards, perhaps. How could consciousness ever arise out of such basic components? Totally. And I think a reason to not trust that is that you can make the same argument about neurons if you started out if you were building humans out of biological neurons, maybe you started out by stringing together some neurons grown in a dish and just a few of them, and like, oh, come on. That's not I mean, I even have this I notice when I engage with little sea creatures or creatures, sometimes they have these tiny, translucent you can maybe see kind of internal into the system. And you look at and you're like, how could that be conscious? Like, I can see all the parts. I don't see any consciousness in there. How could that happen? And literally, there are these thought experiments in philosophy where you imagine touring around inside a brain or you imagine a giant network of humans, like passing punch cards and all sorts of stuff. Now, some views of consciousness do say no, there's something special about this such that computers can't be conscious or whatever. I'm really skeptical. I think it's just very likely that kind of digital minds can be conscious in the sense that I care about, but I think people are going to have trouble with it on that level. On the other hand, and so if you compare with, for example, animals, animals don't struggle in that regard. I think people have it's easier for people like, well, animals are kind of similar to us. They're made of the same stuff. They have brains sort of like us. So it's sort of an easier question or you don't have to sort of do that gap. Yeah, if they are evolutionarily close to us. So if they kind of look like us, and if they are perhaps big enough for us to take them seriously. Just a personal example is that I can look at a shrimp of a certain size and think that's probably not conscious, but then look at a larger shrimp that are not more complex and think that, well, that's obviously conscious. And so I just don't really trust my intuitions on this topic anymore. Totally. I think we're lost on this. It's a really bad intellectual situation and something I think we should be kind of approaching with a lot of kind of fear and trembling because we are currently playing with we know the stuff that matters most, or really core to our conception of what matters. Is this something about minds and what's up with minds. And we're just like throwing mind stuff around. We're just like building random new minds and stirring it with a bunch of grainy descent and putting it in whatever situation we want. We're taking the stuff that you make souls out of and kind of just like kind of mad scientist Flailing Alchemy we have no idea what we're doing, but we know that the stuff we're touring with is nearby, the stuff that is most precious and important. And I think that's like a really scary, really scary situation from an ethical perspective. Yeah. And I think we're going to get it. I think in contrast with the sort of difficulty of imagining digital consciousness, which doesn't apply to animals, I think the thing that AI is going to have in the opposite direction is it's going to be smart and is going to talk and is going to be able to like you're going to interact with it in a bunch of ways that you're used to interacting with systems that you're recording kind of moral status to. It also might be optimized in other ways and in the limit you could even have, and this is something I worry about, you can have AI systems that aren't conscious or that don't warrant kind of moral consideration. By our lights, but that nevertheless pretend to in order to gain power, or don't pretend to, and maybe they really do, but they're trying to gain power, and they're trying to kill you. Just because something has moral status doesn't mean you should let it kill you. It's going to be a really messy situation, I think. I worry about kind of hard trade offs here. I think it's sort of a whole additional feature of the situation that doesn't get, in my opinion, kind of enough attention. But in particular, the thing I worry about, going back to your question about scenarios, where sort of middling scenarios, I worry that we get this part wrong later on, and I think there is just a scary track record of humans not giving due consideration to kind of sufficiently different forms of kind of sufficiently different creatures. You can just easily imagine us treating AIS kind of in ways we have mistreated humans, in ways we have mistreated animals. I'm hopeful that if we can sort ourselves out as a civilization and kind of understand things better, kind of get safer from an existential risk perspective, then we can sort of appropriately respond to that category of considerations. But I don't think it's guaranteed, and I think it's like a scary way that's sort of middling where we have a sort of, in a deep sense, like, unjust or kind of bad society for digital minds in a longer term way. This question of digital minds, if you say you were talking to a person who just thinks this cannot be an issue, this is so far out that it doesn't make sense to even theorize about digital sentience, what's your way of framing this to make it seem perhaps a bit more plausible? Why do you lean towards perhaps computational theories of consciousness? Or why do you find it plausible that we could have digital sentience? Probably the argument. So there's a few arguments that people sort of bandy around that I'm kind of reasonably sympathetic to. Maybe the easiest, though it's somewhat complicated if you want to really get into the physiology. But at a high level, we can do these sort of gradual replacement scenarios where we imagine kind of all right. The high level question was, like, could we build a mind that is in some sense computationally like yours, or structurally like yours out of non biological elements? Right? And I think it's very, very plausible that we could we can do a whole brain emulation in the limit of sort of this really intensive kind of simulation of your brain, and it will reproduce all of your verbal reports and all of your kind of even the internal processing will be kind of structurally isomorphic across your brain. And this emulation not necessarily one for one, because it'll like chaos, but it's like it'll be the same kind of computational system. At a high level, I think it's just very plausible that that thing is conscious even if we don't talk about transitions. And one way to get that is when you think about what is ultimately driving, when you look inside your mind and you go, I'm conscious, there's something that you're sort of reacting to or detecting, or there's something that's driving what is eventually your literal mouth in the physical world moving, saying, I'm conscious. And that process is sort of a computational one. Or we understand there's a sort of an algorithmic kind of description of why it is that your mouth ends up doing that. And then that description will be preserved kind of by hypothesis with respect to the emulated system, such that in some sense, the system will be saying it's conscious because of the same reason you're saying you're conscious. If you think that the reason you're saying you're conscious is because you're conscious, and this system is saying that it's conscious for the same reason you are, you should think that something's preserved there. Now, I think that's kind of a complicated and abstract argument. I think it actually gets into a bunch of really difficult stuff in the epistemology of consciousness. The somewhat easier argument is to imagine now gradually just making yourself into a system like that. So you sort of replace one neuron with, like, a quite sophisticated, non biological neuron. And some people are like, oh, but it can't just be a simple element. And there's some sort of, like, work with me here. People like, come on, is this really about the proteins? And people like, maybe it is about the proteins. I'm like, all right, well, now we just have a kind of complicated neuroscience question. But it just doesn't seem like this is going to be where the kind of meat is philosophically, but some people disagree. But I sort of think now imagine you change one neuron into a kind of computational element, change another one, change another one. And there's some notion that you're not going to notice. Certainly by hypothesis, you're going to be saying all the same stuff, like the computational structure is preserved. And so it's kind of weird that there's this argument about like, well, is your consciousness kind of fading as as we add more and more neurons? And there's some thought that, come on, man, no way you're going to be like, half conscious through it. And some people are like, well, I don't know, p zombies. Anyway, it gets very weird. If you want, we can just talk about, like, well, whatever. We have expert surveys. It's like, I don't know, they're like 60, 80%. I think the experts are too low on this. I think it's like, more likely, perhaps the thing that's worrying here is just that we won't be able to distinguish between sophisticated mimicry of consciousness and actual consciousness in AIS. And so we might be misled into giving AIS rights that shouldn't have rights, or we might, from our gut reactions, be misled into giving AIS that should have rights, not giving those AIS rights. And so it gets very difficult all of a sudden. I've discussed this question of digital sentience on this podcast before. One of my big questions here is just what on earth can we do about this? For the alignment problem, it seems at least somewhat graspable there are different approaches, interpretability work. For example, what on earth could we do about digital sentience in the limit we can understand? You do a bunch of philosophy, you really understand the neuroscience. I think there's some worries that the very question is confused. But I think at a basic level we can just understand all the relevant facts and then sort of act in light of that understanding. Eventually we are going to be very far from that, in my opinion, for a kind of importantly substantive period of time, depending on kind of how focused you are on the longer term versus kind of nearer term stuff. We are not, I think, in the near term going to know what's going on with our systems. We're not going to know what internal properties they have. We're not going to know what internal properties we care about. I don't think we should assume that we only care about consciousness. I think it's possible that you can mistreat in ethically relevant senses non conscious systems, which is a sort of controversial view. I just think we should be quite open to just tons of different ways this could shake out in terms of what is ultimately going on with minds, what ultimately is at stake by our lights. In the meantime, I'm most interested right now in what are the low hanging fruit, what are the kind of things we can do that are going to be not requiring of us solving a bunch of philosophy or getting a bunch more insight into whether our systems are conscious, but that are kind of reasonably robust, hopefully cheap, and then we can talk about the expensive ones later, but start with the cheap ones that will kind of enable us to kind of do better by our AI systems. One suggestion that's been made there's a paper bostrom and schulman that I think is kind of a good candidate for kind of low hanging fruit is to save various of the AI systems that we're using and in principle could be mistreating for possible compensation later. Like if we realized that these systems were in some know, we were kind of doing wrong by them or kind of hurting them or something like that. But we can later. Then we can be like, all right, we're going to make sure that your life is amazing on Net, whatever your life, whatever that means is, but in some sense, find a way to make whole. To whatever degree you can such that these systems would be really happy on net with kind of their lives and their situation relative to not existing at all or something like that. Now. I think that's pretty janky and obviously you can ask questions about like, well, what is the actual kind of structure of these systems? What are we assuming their concern is for their future of their life or whatever? I'm not sure, but I think this seems to be relatively cheap though we can talk about it does matter, like how many things you're saving and which things you're treating as distinct entities and we're going to be confused about that too. But that's like an example of something that could just be implemented now and I think could make a difference. Perhaps the impression listeners will get from this conversation is that at least from the first part of the conversation is that we are updating, or a lot of people are updating towards perhaps AGI arriving sooner than they might have thought and perhaps AI risk being higher than they might have thought. There's also a group of people who have been talking about AI risk for decades now and who have been saying that this risk is very serious. You talked about before the Bayesian constraints, about how we can update when we have given some probability of risk. And you said that it's difficult to update from a very low number and I guess that also holds for updating from a very high number of probability in AI risk scenarios and then ever becoming convinced that this is not a big problem. How do you think about the so called doomers and how their epistemology works in practice? There are many, many more people who are out there expressing kind of very dismissive attitudes towards AI risk who will even say kind of and I want to actually really just thumbs up people who are willing to go out there and say a number as opposed to just sort of being dismissive. But people will say one in a million. There are many, many fewer. In fact, I'm only aware of one person, and it's Elie Azer Yakowski, who doesn't actually like to give numbers. Though depending on which blog posts you look at, some of them sort of have quantitative implications that suggest we're at 99.99 something such that we're really effectively 0% on his model. And then it's a question of like, well, maybe his model is wrong, what probabilities he put on that? So there aren't that many people who are like one in a million that we'll be fine, or one in a million that this doesn't happen, but if there were, then it would all the same dynamics would apply. And I think they apply in milder cases who are people who are at 99% that we die. You can just say, well, okay, well that predicts that you're at less than 10%, that you'll ever be at 90%. I think that's right. All the same dynamics will apply. And people should kind of question like is there anything that could give me hope, anything that could drop me down to 90 or even to 98 if I'm at 99 and then ask what is the probability that I end up seeing that and making that update and it'll be the same constraint. How do you think about the possibility that all of this talk of AI risks that you've spent a lot of time on, that I'm spending a lot of time on, that somehow it doesn't matter or it's not real, it turned out to not be a real thing this whole time. How do you think of that possibility? So I think there's a couple of different ways that could happen. Some of them are just like in the model, right? Where if you have, let's say you have some probability on really kind of relevantly, powerful and agentic AI systems being developed within blah time period, but you also have probability that doesn't happen, then it could be, as you're saying, there's like, maybe a chunky probability that you get to the end of your life and you're like, yes. Never happened. And in some sense that was okay, this was all kind of and then as a question like, well, what is your retroactive assessment? But it could be that you were just like, yeah, I made a reasonable x ante call. I looked at the evidence. I think my relationship to the evidence available at the time was reasonable. But nevertheless, things played out in this particular way where that one doesn't I mean, it's tempting to think that if something where you're like only 20% on it ends up happening, that you're like, oh, I must have been super duper wrong. And to some extent you were wrong, but you weren't like that wrong and you might not have been going that wrong in your relationship to the evidence at the time. I think those are the most likely ways in which this was all confused. It's just sort of like an empirical scenario that and again, this does matter what sort of doomer you are. So if you're at 99% and you have this super confident model of exactly how this plays out and exactly how hard things are, then I think it's a lot easier to have ended up super wrong. And I think sometimes the discourse is sort of dominated by the most confident voices on either side. And so it can really seem like well, there's a sort of strict dichotomy, whereas I think, in fact, the sort of more reasonable probability distributions just have decent weight on a lot of different scenarios that are kind of variously degrees various degrees of doomy. And so that's like, my best guess is that there's just different ways this could play out. Alignment could be not that hard. Humans could coordinate in various ways, like takeoff could be slow, there's stuff like that. And then there are kind of more fundamental confusions where later we look back and we're like, we were just like, thinking about this all wrong. That one I think is like my best candidate for that is we were kind of like over focused on some notion of agency and kind of somehow thinking about things in terms of their objectives too much. Like we had sort of raified our kind of intuitive human modeling of other agents in ways that misled us. As I said before, I don't think that's sufficient to make it confused to think about a paperclip maximizer or confused to think about just like a kind of agentic system. But I think it can lead you to kind of privilege that framework as a way of predicting what future AI will go. And I think that's like a candidate for a confusion that the discourse has suffered from, or we will look back on and be like, we were confused about those concepts. Yeah. So one part of the question of thinking about AI risk is thinking about the increase in AI capabilities. And this is also something that's part of your report on power seeking AI as a potential existential risk. What are the best tools we have for measuring AI progress or AI capabilities? You see, for example, GPT four passing high school exams, getting A's in high school exam, passing the bar exam. What does this mean? Right. Does this mean that GPT four is now suddenly as capable as a human lawyer? No, we all know that there's something to being a lawyer that's not just about passing exams. Yeah, we have all kinds of candidates we could run through. But perhaps let's start with talking about benchmarks. How do you think about benchmarks and comparing AI performance to human performance by that metric, other things equal. I think it's great to have benchmarks because you can really track what's going on and it's a kind of quantitative thing and you can optimize for it and stuff like that. I do think there's a general history of it being somewhat difficult to specify ahead of time the tasks that will be blah degree meaningful with respect to AI progress. I think a classic example here is people thinking that chess was this sort of really important indicator of kind of deep intelligence. We shouldn't underestimate how much weight people put on chess as kind of like the upper end of human intellectual achievement before it was basically a solved problem in AI. The thing I do want to say, to the extent I've been kind of encouraging this mode of like, well, look ahead to what evidence you can get and forecast what will you feel then? I think that's virtuous and good, but you can be kind of wrong about what else you will have learned by the time you get there. Right. So it might have been in the past that you said like, oh, well, if the AIS can play chess, then I will absolutely freak out. Right, but you were imagining the wrong. Or I guess a way of putting is like most of your probability mass was on scenarios that were, in fact importantly different from the one that you end up in, but not with respect to the chess parameter in particular, which is the one that you were articulating. So you're maybe imagining the AIS are doing all sorts of other stuff, but in fact you show up and you're like, the AI sucks. You can see the algorithm, and the algorithm is sort of this brute force thing or whatever, and you're kind of like, oh, I see. I was wrong to privilege this as a kind of indicator of AI performance in general. And I think if that happens, then that's true, or then you take that seriously. But I do think there can be, like, moving of the goalpost things where it's like, well, you did say you thought that was a big deal, and it is here. Had you told me ten years ago that AIS could pass high school exams and even bar exams and so on, I would have been incredibly impressed. I would have called it AGI at that point. But now somehow I'm less impressed. So I can kind of feel myself moving the goalpost in real time, because then I'm thinking about perhaps high school exams aren't actually capturing what we mean by competence in a certain domain and all of these things. How do you think about the things that benchmarks cannot capture specifically in the economic sense of which perhaps what it means to be a lawyer in the real world is to have a network, is to have some human connections with people. And it's not perhaps as much as we might think it is about drafting documents. That task that AI is pretty good at now. And this, of course, connects to how much economic impact AIS will have in the short term and long term. So one aspect of what you're saying there that I think is important and useful is to figure out what is it that you ultimately care about? And I think people sometimes privilege some notion of like, well, when will the AI be real AI? When will it have the special sauce? When will it have that thing? Intelligence, general intelligence. If you're doing that, then you're kind of in for it in terms of like, what did you mean? But B, it's not clear why it matters. It's not clear why it matters. What we dub intelligence or understanding or the sort of verbal debates about which folk theoretical terms we will apply to our AIS, when I think it's better to say what's at stake in those terms, why do we want to know whether those terms apply? And I think economic impact is a very good point of focus there. I do think a difficulty there is there's a bunch of additional stuff at stake in what sort of impact a given system has on the actual real world economy beyond its sort of ability to have that impact. So you might have regulatory barriers, you might have slowness in adoption. There might be other sorts of frictions. But yeah, I mean, other things equal, I think, are people literally actually using the systems for a given task is sort of the most important thing because what we're ultimately wondering about is systems that are doing stuff in the world and able to actually get meaningful and kind of high impact things done. So I think probably the most important benchmark or indicator of kind of scary AI progress from my perspective is the usage of AI. Systems to automate AI R D. Not just necessarily kind of in principle could they do it, but to what extent are existing systems actually being rolled out within AI labs, within hardware companies and improving productivity kind of changing the pace of progress? Reallocating labor? What is the impact of these systems on the AI. R. D process, not necessarily in the economy as a whole, that's I think, for me, the most important thing to be tracking because then we might get into some kind of self improvement loop. Perhaps I'm not thinking about in a very short term loop, but perhaps over years, as AI becomes more and more capable of increasing R D in AI. So why is it that AIS being able to perform AI R D is so important? The scariest scenarios are ones in which there's a kind of feedback loop. There's a very kind of traditional story that I expect to track the kind of dynamics less directly, which is this sort of AI is literally kind of editing its own source code and kind of in this very kind of recursive self improvement dynamic that is sort of driven by the AI itself. But there's a pretty nearby dynamic that I think is worrying for kind of similar reasons, wherein the kind of whole process of kind of improving our AI systems and all of the inputs that drive AI progress become the labor involved is kind of more and more fully automated and the kind of outputs are being reinvested. We're getting software progress driven increasingly by kind of AI, by automated AI researchers who are then kind of improved by that progress. We're getting hardware more efficient chips that are being kind of designed by AIS themselves. There's a question eventually you're automating the actual literal hardware production now that's, I think a longer process. And so the scariest scenarios, I think, involve actually a kind of more purely software focused feedback loop, at least initially. And then the worry is if you kind of really close the loop and you have a kind of fully automated opening eye or even depending on how things go. But once you get that, then it's like, I think you're in a pretty scary scenario. And there's a report by my colleague Tom Davidson at Openfill that goes into that in some detail, which I recommend to people. So I see that as the factor that drives AI progress the most. And also I think it suffers from fewer of the larger kind of barriers to implementation. They're not entirely free from them that come from economy wide adoption of various forms of automation. I think that's just like a much longer process, there's a bunch more friction and hurdles, and I think it's actually not necessary to end up with kind of pretty scary degrees of AI. Capability. It's the case, for example, that AI. Researchers are pretty close to what it would mean to have AI help with AI. Research. So that's perhaps one argument for expecting AI R D to be automated before other types of endeavors. Is it also the case that there's just a huge amount of money to be made because AI. Researchers are so expensive that automating some part of their job would be very economically valuable? Are we perhaps living in sort of a weird world in which AI. Research and development, which I would expect to be something that's automated at the very end, along with perhaps physics research or something that seems to me like very advanced stuff, that that type of research is automated? Before we see automation in other parts of the economy, we should distinguish between the sort of capacity to automate it being available in the sense of, like, the AI. Capabilities themselves being available if you wanted. In practice, do people take the step of implementing and of implementing the automation in question? And I think my guess is that by the time you're actually able to fully automate AI R and D process, you're also able to automate most stuff just because I think the AI. R and D process involves kind of most especially the cognitive labor. I think there's a sort of additional question about what goes on with robots and how hard is it to kind of get your kind of physical labor. But speaking just to all the aspects of what people at OpenAI do that involve just like, their laptops and clicking around and stuff, I think once you can do that, you can probably do also most of what the physicists do. There's some question maybe you might wonder about especially specific forms of cognition that certain very specialized humans are doing and will that take a different but broadly, I expect the capacities to come online at around the same time. And I actually wouldn't expect physics to necessarily be like a super difficult thing to automate. The things that seem harder for me is like, when are you going to get AI teachers or surgeons or domains where we're really quite kind of heavily regulated industries, where we're going to have really intensive checks and there's going to be a big political debate and are you going to trust the people? And all sorts of stuff. That's sort of more what I'm talking about. I think there's, like barriers to society rolling out and adopting and trusting these systems whereas well a there's going be to more comfortable the systems b they can literally I think it's the case that these labs will often deploy their products before they release them to the world. They'll be using them internally and employees will have access and then yeah, I think at a certain point if AI is a sufficiently valuable driver of kind of growth and profit then you just want to be reinvesting or really focusing on that. Unless we regulate right now it's just a lot easier to kind of deploy these things. There isn't some big hassle or not as much of a hassle to just using the chatbot that you built internal to your company than there is in making it a doctor. Yeah, we should say this is happening to some degree already. I would expect at least to have top labs using Copilot basically which is an OpenAI originated product, which is an autocomplete for coding or programming. And so in that sense we could see perhaps signs that we will see more automation in AI research and development. Yeah, I think code is a really important place to look like how much are kind of coding assistants speeding up the kind of coding process and what aspects of it. But I do think there are other things too. I think probably for this to work you need just like a really holistic assistant of the type that people are trying to build but that can kind of assist you with just all sorts of desktop activities and coordination and communication, all sorts of stuff. So I do think more than code is required, but at a certain level once the AIS are like coding as well as human coders, like the best human coders at OpenAI and once the AIS are now and now they're generating ideas for experiments and designing experiments and they're generating new algorithms. I think once the AIS are developing algorithmic breakthroughs as kind of important as the transformer, now we're really cooking with very scary gas. That's a really intense degree of kind of innovation coming out of these systems. If you can then really step on the pedal with that where you go. So I think that's scary stuff and thereby especially important to track. Yeah, I think when I think of programming perhaps I think of it as a bit of a lower level activity, closer to the details. For some reason I find myself thinking that AI would have trouble automating the role of a CEO. So not inventing an AI breakthrough at the level of a transformer, but saying that okay, now we've seen the transformer paper, now we're going to invest heavily in this area. We're going to use resources here and not there. We're going to make these strategic decisions, big picture thinking, long time frames and so on for the running of a company or lab like OpenAI or DeepMind or so on that would be required or that is required now. And so wouldn't that be a barrier to AI automating AI research and development? I think it would. I mean, in general, I think there's some scenarios here that focus specifically on a sort of important phase transition that occurs when you move from 99% automation to full automation such that now OpenAI like the humans are just watching OpenAI kind of surge forward or something. I don't think that's necessary to get the thing that I'm scared about. In fact, in Tom's model, once you have systems that can really kind of fully substitute for human cognitive labor, you don't even need to kind of boost their capabilities all that much further to end up in a very scary situation. In particular, he has these calculations where if you look at the sort of amount of compute that is sort of realistically at stake in training a system of that kind, then once you've trained that system just with the compute that you had of access, you'll be able to run. I think he has some calculation where it's like if compute is if it's like GPT six or seven or something, I think it's like 100 million human equivalent workers. Like in OpenAI you will have 100 million human equivalent workers. Something. It's like, okay, well now I think we can talk a little bit like what are the memory requirements? How's that actually work, how's that calculation work? But the broad idea that you'll suddenly, or maybe not suddenly because you're going to have built up to this you have this glut of high quality cognitive labor. It doesn't need to sleep. It's probably by the time it can kind of do most of the stuff or really fully substitute for human cognition, it's also way better at tons of other stuff. You can rapidly improve. You can try to get software innovations out of this. I think it's very plausible that you can get into the regime where we have AIS that could really where if we lost control of them, it would be disastrous without having any sort of full handoff by the humans to a fully automated company. It's sort of a question of just are there sufficiently many sufficiently capable systems that if we look around the world or maybe we don't even have a chance to decide whether to deploy them if they really start kind of taking the reins. But I think in general there can still be humans in the loop. By the time we die. Though, if humans are out of the loop entirely, then it's especially easy to imagine that things go off the rails. So thinking in terms of scenarios like this make it clear to me that this could happen much more quickly than I might expect. And you see rhetoric coming out of, again, top labs or top companies thinking about this decade as being especially important. What do you think we can do or should do if it is the case? Let's just assume that we have something like transformative AI by 2030 or 2035. What do you think we should do in that scenario? Should we perhaps focus in a hardcore way on specific projects and drop everything else? So if we knew that we were going to get kind of transformative AI by 2035 or 2030 at a civilizational level, I think if we really knew that, and I also think this given to some extent, the amount of probability we should have on that, I think there's actually just quite a big burden of proof on tons of other projects. So I'm sort of sympathetic to, for example, how much theoretical physics do we really need to do in the next decade? I'm kind of like all of the physics students, if they're physicists who are professors who are listening to this podcast or whatever, I'm a little like, if you really knew. So one thing is all of your research, the AIS are going to do it immediately. They're going to get all the answers, like all the things you didn't get, you're going to be out, and you'll learn the answers later. But how cool is it to have made whatever incremental progress you'll make in the next little bit? There's this general image in Nick Brostrom's work of, like, if you're digging a hole and there's like a bulldozer that's coming, you might not want to focus on what is your marginal solution to digging the hole? You might want to focus on what's going to happen with this bulldozer. Right. And what sort of hole is it going to dig and is it going to dig you into the hole? I don't know. I'm messing with the analogy. So I think there's a bunch of stuff that seems to me sort of in that vein in terms of human intellectual progress. I just think there's a bunch of questions that we don't need to answer to deal with this thing. And then once if we deal with this thing correctly, then we'll get this big glut of other forms of progress. I think I want to be wary of sort of broader forms of emergency mode, like drop everything, especially in your personal life or other things. I think that we should just be more wary than I think some people are initially about kind of really how much is the costs of kind of totally upending everything and being like, I shall burn all my resources and pivot everything. And I think that applies at a societal level, and I think that applies at a personal level too. Exactly. We have some stories of people in 2017 thinking that perhaps AGI was coming within the next five years and cashing in their retirement and all of these sorts of things. And that paradigm back then, the paradigm was more something like reinforcement learning agents in bigger and bigger environments becoming more and more capable until they could just be deployed in the real world. And I don't know how that paradigm is going, but five years has gone by, and we don't have transformative AI. So there are some lessons to be learned from that. I think even under very short timelines, it probably doesn't make sense to kind of uproot your life and change everything just because of the uncertainty involved. Now, I specifically asked you to discuss this as just a certainty that we will have transformative AI by 2035, say. And so, yeah, that's something to keep in mind. I think if you knew that the person in 2017 who knew that AI was coming in in five years yeah. Why are you saving for retirement? I think that's a totally reasonable response, and I think you should, in some sense, be if you're 50% on that or whatever that might affect your retirement planning, it might affect how you think about a bunch of stuff. I do actually think we shouldn't. There is a real balance where if you actually believe this, it's not just like some weird discourse that you nod along at, then sometimes it actually can affect your literal expectations of, like, if you have kids, what will happen to your kids? What will your life be like? What should your kids study? What should you expect to happen with all sorts of basic institutions? I think we need to actually ask those questions. I think society is going to change. I think stuff is going to be different. And if I don't want to just compartmentalize too hard and say, like, oh, sure, change what you talk about on Twitter and maybe change what your career is, but don't change anything else that you expect to see in the world. There are ways of compartmentalization that I think are kind of looking away from stuff, and then there are forms of compartmentalization that I think are healthy. I think it's okay to just even if you really think that this is happening, I think it's like, okay to just have parts of your life where you just don't think about it at all. And you just hang out and you do things that you enjoy. You hang out with your friends, you have a family, you do all sorts of stuff. I think that is more true to a later, even as you get closer than I think people often think. People sort of think, oh, my God, if something's going to happen in five years, I should start burning my candle at both ends now. And I sort of think marathon, not a sprint, applies for a lot of lengths of running, and there's a bunch of costs to burning resources that people don't see and other things. You can't sprint for ten years, perhaps, is what you're saying here. Even if timelines are very short, you can't sprint the whole way. You will burn out. That's right. And I also do think in the context of uncertainty, I think we should be just really wary of ways in which kind of emergency inflected memes can kind of commandeer your resources and tell you to stop thinking, tell you to stop being reasonable. Don't question it. Just act on it. I'm really skeptical that I think people should be skeptical. I think people should take the time, even if the AI safety discourse is like, don't think about me. Don't bother to assess the arguments. Just assume I'm right. I'm like people who tell you, don't bother to assess my argument, but just assume I'm right. I'm like, come on. I don't think we should be in emergency mode to a degree that makes us kind of intellectually blind or kind of insufficiently skeptical or insufficiently discerning. I think we should be still, in some sense, amping up even more our degree of epistemic awareness, clarity, giving ourselves space to kind of be as sane as possible in the midst of something that can be looks like it might be quite kind of intense. Yeah. Let's stay in this framing of transformative AI coming within the next ten years, say, or 15 years. What do you think in that world? What safety approaches are you most excited about? So I am most excited about safety approaches that apply to the types of systems that we're building today. So I think there's this paradigm, broad paradigm called kind of prosaic AI. I think you should especially we're conditioning on short timelines. You should be like, all right, I am trying to figure out how to align the literal types of systems we're building right now in that bucket. I think the stuff that I'm most excited about is sort of generally under the heading of kind of scalable supervision or scalable oversight as a first pass. Right now, the way we make these systems, you sort of start out with this really alien thing that we sort of got a glimpse of with bing, and then we do this process called rlhf, where basically you have humans kind of express their approval of the model's behavior or sort of express some opinion about it. And then you train a model to predict what the humans will say, and then you use that to train the sort of base model to kind of behave better. Now, that works if humans can evaluate the behavior, right? But the more the models start being capable of understanding stuff humans can't understand and doing stuff in domains that humans can't track or kind of just sort of the more autonomous and complex and kind of sophisticated AI cognition and action becomes, the harder it is for a human to just look at it and go, like, thumbs up, thumbs down. It's more like, what the heck is that? It outputs this giant code base, and you're like, okay. And it's like, here's my proposal for nanotech. And this nanotech will cure cancer. Just like, build it. And you're like, oh, my God, is this a thumbs down? Is this thumbs up. What's going to happen with this? We need at a basic level. As we start to have superhuman systems, the sort of current paradigm of supervision is going to become less and less capable of constraining the behavior of the systems we're trying to supervise in ways that we would kind of like if we really understood the paradigm we have for aligning systems that are not as smart as us possibly will not extend to aligning systems that are much smarter than us or just smarter than us. Yeah, I think a lot of the hardest problems here come specifically from what does your alignment technique do with a system that is way smarter than you? And I think there are kind of somewhat important differences between or sorry, there are really important differences between supervising a system that you're smarter than and supervising a system that's smarter than you or kind of understanding a system that's smarter than you and stuff like that. And I think in general, people just really need to be asking of their alignment techniques, is this scalable? The scalable part is really important. So I'm really interested in that. I think separately there's a whole bucket of research into kind of threat modeling and kind of demos where we have this set of concerns. I think a lot of these questions are increasingly empirical questions. I think some people, the really hardcore doomers, think that the dynamics here are sort of derivable operi, that you can just really, without necessarily having seen how this goes, you can kind of know from some kind of suitable probability distribution over the space of possible goals or the space of possible minds that could satisfy your Is criteria that these minds will be deceptive and kind of have different sorts of different types of structure and agency and stuff like that. I think a lot of those questions are actually much more empirical. It's definitely a live hypothesis. It's a worryingly live hypothesis that you get a bunch of these kind of bad forms of behavior by default or in various kind of salient contexts. But we, I think, don't yet know how hard it is, how often it crops up, how hard it is to deal with how different techniques will work. I think we need to be just like getting data and doing experiments and kind of starting to really kind of nitty gritty encounter these issues as they actually play out in the kind of empirical world and start to understand that just like getting demos of when do you get problematic forms of power seeking? When do you get forms of deception? Like what sorts of agency do you see in different circumstances? How do systems cooperate? How do they generalize? There's all sorts of just kind of ways we can look at the specific issues we're worried about with alignment and kind of see how they show up and then if they show up in scary ways to tell people. Really. And then if you see an instance of this scary behavior, really study it, really vary different parameters. How often does that come up? What's up with that behavior? So I think there's a bunch of those are the things I'm most excited about. And then there's some other stuff with interpretability I think is also great, but I think is harder on shorter timescales. Yeah. Is there a really kind of dark and ironic world in which we, in order to test whether systems are deceptive or power seeking, try to engineer this feature or this capability into them and thereby destroy ourselves? Is it perhaps dangerous to experiment with trying to get empirical data on these behaviors in AI systems? I'm thinking somewhat analogously to perhaps gain a function research in viruses. Yes, I think the answer is yes. I know people who are grappling with this very dilemma as we speak because there is a way in which you're doing gain of function research. You are trying to make these systems scary or you're trying to bring out their scariness. But bringing out their scariness has hazards both in terms of the consequences for the system's behavior. Maybe if you try to see like, oh, can I build a bioweapon or something like that? And you also can prove out to the world that this is possible or kind of make more salient these failure modes in ways that kind of make them more likely as instigated by humans. So I think there is a lot of difficult stuff there, but I think we do nevertheless need to be finding ways to kind of understand these behaviors, elicit them in safe ways, and kind of learn how they can be addressed and kind of what the impact of different techniques for addressing them actually is. Yeah. You mentioned interpretability research, which is this area where we're trying to take this black box AI system, this alien mind as you described it, and then seeing which algorithms are running, trying to produce something that we humans can understand about what's actually going on under the hood there. How optimistic are you with this? When I talked to Neil Nanda, who's a big proponent and practitioner of this approach, we talked about whether interpretability can keep up with the speed of AI progress. And that being perhaps the biggest open question. Yeah, I mean, I am quite worried about that dynamic kind of obviating the relevance of interpretability research to at least kind of very short timelines scenarios and possibly sort of somewhat medium term scenarios as well. And the reason for that is you just look at where are we at right now in terms of how much have we understood about these models and kind of how hard is it? What's the vibe in terms of what amount of progress is being made, the techniques being used? And you're really looking inside of we're kind of at square one like and you're poking in the neurons and we're learning. It's a really cool domain. I think it makes a ton of sense to be really excited about this. I think if I was just like a generic scientist, I'd be like, oh my God, it's like neuroscience. I think all the neuroscientists should just be freaking out because neuroscience is so data bottlenecked. It's so kind of difficult to do experiments. It's like this really costly thing and now you have these brains that you can just do arbitrary things to and see arbitrarily inside them. You can build new ones. I think from a scientific perspective, interpretability is awesome and it's nice in that respect in the sense that it's quite clear from getting more people to work on this. It's a relatively direct, like, just understand how these things go. You can kind of tell what progress is. It feels very normal science ish. It doesn't feel very like now, of course, there's specific types of interpretability that are more interesting from an alignment perspective. Like, can you understand whether the model is lying to you? Can you understand what the model knows? Are you able to kind of tell extract knowledge from the model that you wouldn't have had? Overall, I think it's an exciting scientific area. I think the worry is just we're at such square one with that and it's not a bottleneck to deployment at all. So I think people can just surge ahead with creating more and more kind of capable systems with approximately zero interpretability progress. So it's very easy for it to get just left behind. That also is sort of a point in its favor because it's sort of very independent of capabilities progress, whereas if you make progress on scalable supervision and Rlhf, it also can unlock kind of more deployment possibilities and stuff like that. Yeah, it just looks to me like in the near term, very kind of hard to see kind of realistic extensions of our current interpretability paradigm, like being adequate to the sorts of tasks that might be asked of them, where you say like, is this model lying to us? Like all this stuff. Now that said, like, everything in alignment, you're hoping to get a bunch of help from the AIS and automate. So it could be that we can automate a bunch of the kind of interpretability process. Both. I mean, this is most feasible though I think also kind of less exciting at the level of like if you have a sort of relatively rote task that you need tons of humans to perform. If there's some part of your interpretability pipeline that an MTurk worker is doing, then I think we should be reasonably optimistic about being able to scale that up fairly hard because I think we should be reasonably optimistic about getting AIS that can imitate like MTurk workers. Once you're doing kind of more complicated stuff or having to make real conceptual breakthroughs, you could also get a bunch of progress. There and I think so that's my most salient way interpretability kind of comes back into relevance is if we get a bunch of AI help and then I do think we want to have AI help on a ton of different levels. Yeah, the pessimistic take there would be then we have some AI interpreter trying to understand another AI system. But how do we know that this AI interpreter system is aligned with our A? We push the problem back one step, we're not sure that we're getting accurate information from the interpreting system. I think that's true, but I also think it can be overplayed as a concern. And this is actually, I think, one of the sources of hope I have more relative to some of the kind of doomer, the more extreme kind of pessimism in particular. I think a lot of that comes from this worry that every system that you're working with is of the type that you're concerned that it's like deceiving you or has misaligned goals or is sort of relevantly agentic or something like that. And then you're worried that these systems, in addition to all of them being of the dangerous type, they are sort of able to coordinate much better than you are. And they're doing like in the extreme case, oh, they can do logical handshakes and show each other their source code and stuff like that. Maybe for extreme stuff, but I'm talking about in the next couple of years. I don't think these models aren't that capable yet. I think if you're able to have supervision from a system that is not an agent or is sort of somehow less worrying or that you trust its output for this specific category of task, then I think you don't necessarily need to be working with things that might all be trying to kill you at once, depending. Or you could have different probabilities on the likelihood that a given system is scary. You can be using less capable systems to automate specific little chunks of supervision or interpretability or whatever. So I think I'm generally more optimistic about can we use AIS even prior to having fully solved the alignment problem? Can we use certain sorts of AIS for certain sorts of tasks in ways that significantly enhance our kind of traction on alignment relevant forms of cognitive labor than the kind of more extreme end of the pessimistic take? That said, it's still scary. You're like, oh, my God. I'm trying to understand. AIS. I'm bringing in the AIS. I'm hoping they're going to help me. It's definitely not like, oh, we've got this done and dusted. But I think there's at least some hope for help there. And in some sense you ought to think that for capabilities, to the extent you think these systems will be useful for capabilities and not just because they've themselves schemed that helping with capabilities will be conducive to their power, but they should sabotage alignment research. I think you're sort of presuming that you've really lost the game in terms of how sophisticated and misaligned are these systems already by the time that's your story about why they're not able to help. But if that hasn't happened, then you might in fact be able to get a lot of help from them in kind of learning about alignment. So we talked about physicists. Perhaps if we're in a world with very short timelines, they shouldn't spend a lot of time trying to solve the most interesting problems in physics. Perhaps they should spend more time on AI. So you've just completed this PhD from Oxford. So I love this stuff. I love the stuff you're writing on your blog. I find it extremely interesting. But how do you think about that side of your work or that part of your work in a world in which AI is racing ahead? How relevant is philosophy in that world? So, I mean, it's definitely a tension that I feel. It was pretty painful the amount of time I spent. I didn't actually spend very much time on the PhD. It's sort of easier to get a PhD if you're not trying to get an academic job and telling the world. And it is, I think, a source of tension in my life. Like, how much should I just totally focus entirely on AI and alignment versus a sort of broader set of projects? The sort of compromise I've used is like the blogging and sort of this other writing is actually like a personal time thing. I try to have a work budget that is much more directly optimizing for just kind of really focusing on sort of what I see as most impactful and bringing this much more optimizers mindset. And there I tend to focus specifically on AI stuff, not entirely. And then in the kind of domain of writing, I see that more as like, this is a time that has many fewer constraints in terms of what I'm expecting of myself, in terms of what sort of optimization I'm bringing to it. And it's more a place of self expression and something that sort of feeds other aspects of my life rather than my what is most important part. That said, I think philosophy matters here. Well in particular, so people bring to some of their orientations towards AI a bunch of extra philosophical assumptions about why this matters. So in particular, I've been interested in long termism. I think there is a nice feature of AI right now, which is I think it's sufficiently convergent as a problem that we don't need to get too into the weeds in terms of are you worried about the long term impacts or the short term impacts or not? I think sometimes that degree of convergence is overplayed. I think in particular, if the AIS don't literally kill you but do something else, then it can start to matter what that else is and what's at stake for present day people, future people. So there's some questions there. I think there are just broader questions where this is, in my opinion, still a sort of deeply philosophically inflected discourse. I mentioned this stuff about the kind of ontology of agency and all this stuff, and this sort of stuff about utility functions and they diverge. And ultimately this is about kind of human values and what's up with human values and what does it mean for there's like a bunch of I think this is a discourse kind of born of philosophy. It's an interesting case of philosophy kind of predating a real thing that we encounter and have to deal with, or digital sentience, as we also talked about. This is an area that comes directly out of philosophy and could suddenly become extremely relevant to the world. So I think if we start talking about what are we supposed to do with digital, I think that is going to be super duper philosophical. And I think it's like really worryingly breaking of our we're going to be off distribution. We're going to be trying to generalize to this radically new world all these norms and concepts both in ethics and also these sort of really amorphous things in philosophy of mind. We're like, we care about what is it to be? Consciousness, preferences, autonomy. There are a zillion concepts where we're going to be looking at these AI systems and we're like I mean, we're already doing this. People are like, does it have preferences? I don't know. Does a light switch have preferences? What's a preference? And that stuff is going to bite really hard as we move into this new domain off distribution. You have to generalize. Philosophy is in some sense the art of figuring out how to generalize, figuring out what your kind of naive concepts were and how you should move them to other areas. So I think there's like a ton of philosophy that's going to be done or need to be done if we can kind of survive. I think a decent amount of that you can kind of defer to the future. And I talk about that in my thesis and other stuff, but I don't think you can defer all of it in general. I think philosophy for me at least, and I hope for many people, is sort of an effort to be a kind of sane and aware and kind of coherent person or soul or human in the world. And I think that project kind of which is a little less about knowledge gathering and more about what is going it's sort of awareness of yourself and kind of poise and orientation that you endorse. And I think that project kind of persists in its urgency even amidst AI kind of taking off. Fantastic. Joe, thank you for all the time you've spent with me. Thank you for coming on the podcast. It's been a pleasure. Yeah, thanks for having me here. It's really fun.